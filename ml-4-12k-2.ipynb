{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bd0ed69e34964858afe9ab74e819fb01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e427d66e66404956b12cc26a56442045","IPY_MODEL_c378da876da24da193c9db15d512e8b6","IPY_MODEL_6bc0850f39ea41718148b93d7cd72b6c"],"layout":"IPY_MODEL_9b9b3b9ef802491d8c151b2213a664c2"}},"e427d66e66404956b12cc26a56442045":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d84a3b5e77824f7fb77f0695416cb07e","placeholder":"​","style":"IPY_MODEL_47310cc9b0d74a6da576de7fab6033ca","value":"model.safetensors: 100%"}},"c378da876da24da193c9db15d512e8b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_94b9d89ebf734c3881eb74375b590144","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4262c43fb8844f2b50f5bcd90241829","value":267954768}},"6bc0850f39ea41718148b93d7cd72b6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9ffd388086e4188afee9de8fd2c5e72","placeholder":"​","style":"IPY_MODEL_92a5af1ef90f4d8d8ffce4e8f8a53f39","value":" 268M/268M [00:03&lt;00:00, 49.3MB/s]"}},"9b9b3b9ef802491d8c151b2213a664c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84a3b5e77824f7fb77f0695416cb07e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47310cc9b0d74a6da576de7fab6033ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94b9d89ebf734c3881eb74375b590144":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4262c43fb8844f2b50f5bcd90241829":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9ffd388086e4188afee9de8fd2c5e72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92a5af1ef90f4d8d8ffce4e8f8a53f39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n!pip install -q transformers tqdm scikit-learn pillow ijson pandas gradio","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xx9a1KmP5L84","outputId":"5e4b548b-b5c1-437e-8b6d-dbe0e9d30578","executionInfo":{"status":"ok","timestamp":1764534025360,"user_tz":300,"elapsed":11275,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\n\nimport os\nimport json\nimport ijson\nimport zipfile\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom transformers import AutoTokenizer, AutoModel\n\n# Define Configuration\n\n# Set Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device set to: {device}\")\n\n# Mount Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Path to the ZIP file in Drive containing 12,000 images\nZIP_FILE_PATH = \"/content/drive/MyDrive/recipe_project/recipe1m_tiny3.zip\"\n\n# Paths to the JSON files in Drive\nLAYER1_JSON = \"/content/drive/MyDrive/recipe_project/layer1.json\"  # Text\nLAYER2_JSON = \"/content/drive/MyDrive/recipe_project/layer2+.json\"  # Images\n\n# Local Runtime Paths\nPROJECT_ROOT = \"/content/recipe_project_12k\"\nIMAGES_DIR = f\"{PROJECT_ROOT}/images\"\nos.makedirs(IMAGES_DIR, exist_ok=True)\n\nprint(\"Configuration complete.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IYUtPHsz5bH0","outputId":"143fa9d0-affb-439d-c7b6-5ec65ff193d4","executionInfo":{"status":"ok","timestamp":1764534344203,"user_tz":300,"elapsed":53210,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Device set to: cuda\n","Mounted at /content/drive\n","Configuration complete.\n"]}],"execution_count":2},{"cell_type":"markdown","source":"# **Data Preparation**","metadata":{"id":"DS_Hfy9EiwYv"}},{"cell_type":"code","source":"def unzip_data(zip_path, extract_to):\n\n    # Check if unzipped file exists\n    if len(os.listdir(extract_to)) > 100:\n        print(f\"Folder {extract_to} already has files. Skipping unzip.\")\n        return\n\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        # Filter to extract only JPGs if zip is messy\n        for file in tqdm(zip_ref.namelist(), desc=\"Extracting\"):\n            if file.endswith(('.jpg', '.jpeg', '.png')):\n                zip_ref.extract(file, extract_to)\n\n    print(\"Extraction complete.\")\n\n# Run the extraction\nunzip_data(ZIP_FILE_PATH, IMAGES_DIR)\n\n# Handle nested folders\ncontents = [f for f in os.listdir(IMAGES_DIR) if not f.startswith('__MACOSX')]\n\nif len(contents) == 1 and os.path.isdir(os.path.join(IMAGES_DIR, contents[0])):\n    FINAL_IMG_DIR = os.path.join(IMAGES_DIR, contents[0])\nelse:\n    FINAL_IMG_DIR = IMAGES_DIR\n\nprint(f\"Images are located in: {FINAL_IMG_DIR}\")\nprint(f\"Image Count: {len([f for f in os.listdir(FINAL_IMG_DIR) if f.endswith('.jpg')])}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd666slZ5tqw","outputId":"5eb9716c-ae8d-4616-e087-caaa6ab7fc05","executionInfo":{"status":"ok","timestamp":1764534423136,"user_tz":300,"elapsed":50835,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Extracting: 100%|██████████| 12001/12001 [00:46<00:00, 256.52it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extraction complete.\n","Images are located in: /content/recipe_project_12k/images/recipe1m_tiny3\n","Image Count: 12000\n"]}],"execution_count":3},{"cell_type":"code","source":"# Get set of available image filenames on disk\navailable_images = set(os.listdir(FINAL_IMG_DIR))\nprint(f\"Found {len(available_images)} images on disk.\")\n\n# 1. Map Image Filename -> Recipe ID (using layer2.json)\nmatched_recipe_ids = set()\nid_to_img = {}\n\nprint(\"Streaming layer2+.json to match images\")\nwith open(LAYER2_JSON, 'rb') as f:\n\n    for entry in ijson.items(f, 'item'):\n        rid = entry['id']\n        for img in entry.get('images', []):\n            fname = img['id']\n            # If this image exists in our 12k folder\n            if fname in available_images:\n                matched_recipe_ids.add(rid)\n                id_to_img[rid] = fname\n                break\n\nprint(f\"Matched {len(matched_recipe_ids)} Recipe IDs.\")\n\n# 2. Extract Text for those IDs (using layer1.json)\nfinal_dataset = []\n\nprint(\"Streaming layer1.json to extract text\")\nwith open(LAYER1_JSON, 'rb') as f:\n    for entry in ijson.items(f, 'item'):\n        rid = entry['id']\n        if rid in matched_recipe_ids:\n            final_dataset.append({\n                \"recipe_id\": rid,\n                \"image_filename\": id_to_img[rid],\n                \"title\": entry.get(\"title\", \"Untitled\"),\n                \"ingredients\": [x['text'] for x in entry.get(\"ingredients\", [])],\n                \"instructions\": [x['text'] for x in entry.get(\"instructions\", [])]\n            })\n\nprint(f\"Final Dataset Size: {len(final_dataset)} pairs.\")\n\n# Save this cleaned dataset\nwith open(f\"{PROJECT_ROOT}/dataset_12k_clean.json\", \"w\") as f:\n    json.dump(final_dataset, f)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mULJ1HEn56GL","outputId":"13139320-eb2b-4b88-8f82-f3ff1dd58314","executionInfo":{"status":"ok","timestamp":1764534503842,"user_tz":300,"elapsed":72453,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 12000 images on disk.\n","Streaming layer2+.json to match images\n","Matched 7100 Recipe IDs.\n","Streaming layer1.json to extract text\n","Final Dataset Size: 7100 pairs.\n"]}],"execution_count":4},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{"id":"rTpNG2msjdxC"}},{"cell_type":"code","source":"# A. AUGMENTATION (The Fix for Overfitting)\n# We use strong augmentation for training to force the model to learn features, not pixels.\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),        # Resize slightly larger than target\n    transforms.RandomCrop(224),           # Randomly crop the 224x224 area\n    transforms.RandomHorizontalFlip(),    # Flip left/right (food looks same flipped)\n    transforms.RandomRotation(15),        # Rotate +/- 15 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Vary lighting\n    transforms.ToTensor(),                # Convert to Tensor\n    transforms.Normalize(                 # Normalize to ImageNet standards\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# For Validation, resize and normalize (No randomness)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\n# B. DATASET CLASS\n\nclass RecipeDataset(Dataset):\n    def __init__(self, data_list, img_dir, transform, tokenizer, max_len=128):\n        self.data = data_list\n        self.img_dir = img_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        # 1. Image\n        img_path = os.path.join(self.img_dir, item['image_filename'])\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n            image = self.transform(image)\n        except:\n            # Fallback for corrupted images\n            image = torch.zeros(3, 224, 224)\n\n        # 2. Text (Concatenate Title + Ingredients + Instructions)\n        # We join them to give BERT maximum context\n        text_input = f\"{item['title']} [SEP] \" + \\\n                     \" \".join(item['ingredients']) + \\\n                     \" [SEP] \" + \\\n                     \" \".join(item['instructions'])\n\n        encoded = self.tokenizer(\n            text_input,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        return {\n            \"image\": image,\n            \"input_ids\": encoded['input_ids'].squeeze(0),\n            \"attention_mask\": encoded['attention_mask'].squeeze(0),\n            \"recipe_id\": item['recipe_id'] # Keep ID for lookup later\n        }\n\nprint(\"Transforms and Dataset class defined.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdN-0ORI6qAv","outputId":"1c341df6-35b1-4a45-efe8-28aaaa0e42f6","executionInfo":{"status":"ok","timestamp":1764534550152,"user_tz":300,"elapsed":45,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Transforms and Dataset class defined.\n"]}],"execution_count":5},{"cell_type":"markdown","source":"# **Model Training**","metadata":{"id":"nexuMPC4jqrJ"}},{"cell_type":"code","source":"# 1. LOAD CLEAN DATASET (12k)\n\nDATASET_PATH = f\"{PROJECT_ROOT}/dataset_12k_clean.json\"\n\nwith open(DATASET_PATH, \"r\") as f:\n    full_data = json.load(f)\n\nprint(f\"Loaded {len(full_data)} (image, text) pairs from {DATASET_PATH}\")\n\n\n# 2. 3-WAY SPLIT: Train (80%), Validation (10%), Test (10%)\n\nfrom sklearn.model_selection import train_test_split\n\n# To Ensure reproducibility\nRANDOM_SEED = 42\n\n# Shuffle entire dataset once\nnp.random.seed(RANDOM_SEED)\nnp.random.shuffle(full_data)\n\n# Train / Temp split (80% / 20%)\ntrain_list, temp_list = train_test_split(\n    full_data,\n    test_size=0.20,\n    random_state=RANDOM_SEED\n)\n\n# Temp → Val/Test split (50% / 50% of the 20%)\nval_list, test_list = train_test_split(\n    temp_list,\n    test_size=0.50,\n    random_state=RANDOM_SEED\n)\n\nprint(\"Dataset Splits:\")\nprint(f\" Train      : {len(train_list)} ({len(train_list)/len(full_data)*100:.1f}%)\")\nprint(f\" Validation : {len(val_list)} ({len(val_list)/len(full_data)*100:.1f}%)\")\nprint(f\" Test       : {len(test_list)} ({len(test_list)/len(full_data)*100:.1f}%)\")\n\n\n\n# 3. TOKENIZER (DistilBERT)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"distilbert-base-uncased\",\n    model_max_length=128,\n    padding=\"max_length\",\n    truncation=True\n)\n\nprint(\"Tokenizer initialized.\")\n\n\n\n# 4. DATASETS\n\ntrain_dataset = RecipeDataset(\n    train_list, FINAL_IMG_DIR, train_transform, tokenizer\n)\nval_dataset = RecipeDataset(\n    val_list, FINAL_IMG_DIR, val_transform, tokenizer\n)\ntest_dataset = RecipeDataset(\n    test_list, FINAL_IMG_DIR, val_transform, tokenizer\n)\n\nprint(\"Datasets constructed.\")\n\n\n# 5. DATA LOADERS\n\nBATCH_SIZE = 32\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=2, pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\nprint(f\"DataLoaders ready (batch_size={BATCH_SIZE}).\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mX53sCG8Qge","outputId":"c899d389-b1ad-465e-ce43-cd2e9812a478","executionInfo":{"status":"ok","timestamp":1764534665109,"user_tz":300,"elapsed":754,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 7100 (image, text) pairs from /content/recipe_project_12k/dataset_12k_clean.json\n","Dataset Splits:\n"," Train      : 5680 (80.0%)\n"," Validation : 710 (10.0%)\n"," Test       : 710 (10.0%)\n","Tokenizer initialized.\n","Datasets constructed.\n","DataLoaders ready (batch_size=32).\n"]}],"execution_count":8},{"cell_type":"code","source":"# ---------------------------------------------------------\n# DUAL ENCODER\n# ---------------------------------------------------------\nclass DualEncoder(nn.Module):\n    def __init__(self, embed_dim=256, freeze_cnn=True, freeze_text_epochs=2):\n        super().__init__()\n\n        self.freeze_text_epochs = freeze_text_epochs\n\n        # ---------------------------------------------------------\n        # IMAGE ENCODER (RESNET50)\n        # ---------------------------------------------------------\n        self.cnn = models.resnet50(weights=\"IMAGENET1K_V2\")\n        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, embed_dim)\n\n        # Freeze CNN initially\n        if freeze_cnn:\n            for p in self.cnn.parameters():\n                p.requires_grad = False\n            for p in self.cnn.fc.parameters():\n                p.requires_grad = True\n\n        # ---------------------------------------------------------\n        # TEXT ENCODER (DISTILBERT)\n        # ---------------------------------------------------------\n        self.text_encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n        self.text_proj = nn.Linear(768, embed_dim)\n\n        # Freeze text encoder initially\n        for p in self.text_encoder.parameters():\n            p.requires_grad = False\n\n    def unfreeze_text_encoder(self):\n        print(\"Unfreezing Text Encoder…\")\n        for p in self.text_encoder.parameters():\n            p.requires_grad = True\n\n    def forward(self, images=None, input_ids=None, attention_mask=None):\n        img_features = None\n        txt_features = None\n\n        if images is not None:\n            img_features = self.cnn(images)\n            img_features = F.normalize(img_features, p=2, dim=1)\n\n        if input_ids is not None:\n            txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n            txt_features = txt_out.last_hidden_state[:, 0, :]\n            txt_features = self.text_proj(txt_features)\n            txt_features = F.normalize(txt_features, p=2, dim=1)\n\n        return img_features, txt_features\n","metadata":{"id":"t6NUrHuT8U3s","executionInfo":{"status":"ok","timestamp":1764534670786,"user_tz":300,"elapsed":41,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ---------------------------------------------------------\n# TRAINING FUNCTION\n# ---------------------------------------------------------\ndef train_model(\n    model,\n    train_loader,\n    val_loader,\n    epochs=15,\n    lr=1e-4,\n    patience=3,\n    unfreeze_cnn_epoch=3,\n    temperature=0.07,\n    save_path=f\"{PROJECT_ROOT}/best_model.pt\"\n):\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=epochs,\n        eta_min=lr * 0.1\n    )\n\n    best_val_loss = float(\"inf\")\n    patience_counter = 0\n\n    def contrastive_loss(img_emb, txt_emb, temperature=temperature):\n        logits = (img_emb @ txt_emb.T) / temperature\n        logits = torch.clamp(logits, -50, 50)\n\n        labels = torch.arange(len(logits)).to(device)\n        loss_i = nn.CrossEntropyLoss()(logits, labels)\n        loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n\n        return (loss_i + loss_t) / 2\n\n    print(\"Starting Training...\")\n\n    for epoch in range(epochs):\n\n        # Unfreeze text encoder AFTER warmup\n        if epoch == model.freeze_text_epochs:\n            model.unfreeze_text_encoder()\n\n        # Unfreeze CNN later\n        if epoch == unfreeze_cnn_epoch:\n            print(f\"Unfreezing CNN at epoch {epoch}\")\n            for p in model.cnn.parameters():\n                p.requires_grad = True\n\n        model.train()\n        total_train_loss = 0\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n        for batch in loop:\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            mask = batch['attention_mask'].to(device)\n\n            img_emb, txt_emb = model(images, input_ids, mask)\n            loss = contrastive_loss(img_emb, txt_emb)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            loop.set_postfix(loss=loss.item())\n\n        avg_train_loss = total_train_loss / len(train_loader)\n\n        # ------------------------ VALIDATION ------------------------\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                images = batch['image'].to(device)\n                input_ids = batch['input_ids'].to(device)\n                mask = batch['attention_mask'].to(device)\n\n                img_emb, txt_emb = model(images, input_ids, mask)\n                total_val_loss += contrastive_loss(img_emb, txt_emb).item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n        # ---------------------- EARLY STOPPING ----------------------\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), save_path)\n            print(\"Saved NEW BEST MODEL!\")\n        else:\n            patience_counter += 1\n            print(f\"No improvement ({patience_counter}/{patience})\")\n\n            if patience_counter >= patience:\n                print(\"Early Stopping Triggered.\")\n                break\n\n        scheduler.step()\n\n    print(\"Training Finished.\")\n    print(f\"Best model saved at: {save_path}\")\n","metadata":{"id":"31RQrqnr4KWr","executionInfo":{"status":"ok","timestamp":1764534700256,"user_tz":300,"elapsed":61,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# TRAINING CALL\nmodel = DualEncoder(embed_dim=256, freeze_cnn=True).to(device)\n\ntrain_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    epochs=15,\n    lr=1e-4,\n    patience=3,\n    unfreeze_cnn_epoch=3,\n    save_path=\"/content/drive/MyDrive/recipe_project/best_model.pt\"\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["bd0ed69e34964858afe9ab74e819fb01","e427d66e66404956b12cc26a56442045","c378da876da24da193c9db15d512e8b6","6bc0850f39ea41718148b93d7cd72b6c","9b9b3b9ef802491d8c151b2213a664c2","d84a3b5e77824f7fb77f0695416cb07e","47310cc9b0d74a6da576de7fab6033ca","94b9d89ebf734c3881eb74375b590144","d4262c43fb8844f2b50f5bcd90241829","f9ffd388086e4188afee9de8fd2c5e72","92a5af1ef90f4d8d8ffce4e8f8a53f39"]},"id":"cRFYcojZ4Vzs","outputId":"b6808d4a-d735-4874-b321-421a3cd7b3f8","executionInfo":{"status":"ok","timestamp":1764536821507,"user_tz":300,"elapsed":2092325,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 97.8M/97.8M [00:00<00:00, 181MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0ed69e34964858afe9ab74e819fb01"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting Training...\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/15: 100%|██████████| 178/178 [01:46<00:00,  1.67it/s, loss=2.25]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/15\n","Train Loss: 3.2864 | Val Loss: 2.9566\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/15: 100%|██████████| 178/178 [01:38<00:00,  1.80it/s, loss=1.74]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/15\n","Train Loss: 2.8698 | Val Loss: 2.7941\n","Saved NEW BEST MODEL!\n","Unfreezing Text Encoder…\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/15: 100%|██████████| 178/178 [01:52<00:00,  1.58it/s, loss=1.21]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/15\n","Train Loss: 2.4472 | Val Loss: 2.1096\n","Saved NEW BEST MODEL!\n","Unfreezing CNN at epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/15: 100%|██████████| 178/178 [02:10<00:00,  1.36it/s, loss=0.971]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/15\n","Train Loss: 1.6011 | Val Loss: 1.7398\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/15: 100%|██████████| 178/178 [02:13<00:00,  1.34it/s, loss=0.757]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/15\n","Train Loss: 0.8783 | Val Loss: 1.6367\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/15: 100%|██████████| 178/178 [02:10<00:00,  1.36it/s, loss=0.141]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/15\n","Train Loss: 0.4858 | Val Loss: 1.5053\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/15: 100%|██████████| 178/178 [02:10<00:00,  1.36it/s, loss=0.178]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/15\n","Train Loss: 0.2960 | Val Loss: 1.4953\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/15: 100%|██████████| 178/178 [02:12<00:00,  1.34it/s, loss=0.137]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/15\n","Train Loss: 0.2036 | Val Loss: 1.4781\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/15: 100%|██████████| 178/178 [02:11<00:00,  1.36it/s, loss=0.0534]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/15\n","Train Loss: 0.1472 | Val Loss: 1.4720\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/15: 100%|██████████| 178/178 [02:12<00:00,  1.35it/s, loss=0.0887]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/15\n","Train Loss: 0.1268 | Val Loss: 1.4537\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/15: 100%|██████████| 178/178 [02:11<00:00,  1.36it/s, loss=0.0452]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11/15\n","Train Loss: 0.1064 | Val Loss: 1.4509\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/15: 100%|██████████| 178/178 [02:10<00:00,  1.36it/s, loss=0.0379]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 12/15\n","Train Loss: 0.0934 | Val Loss: 1.4408\n","Saved NEW BEST MODEL!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/15: 100%|██████████| 178/178 [02:11<00:00,  1.35it/s, loss=0.0356]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 13/15\n","Train Loss: 0.0810 | Val Loss: 1.4467\n","No improvement (1/3)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/15: 100%|██████████| 178/178 [02:04<00:00,  1.43it/s, loss=0.0327]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 14/15\n","Train Loss: 0.0769 | Val Loss: 1.4546\n","No improvement (2/3)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/15: 100%|██████████| 178/178 [02:05<00:00,  1.42it/s, loss=0.0264]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 15/15\n","Train Loss: 0.0709 | Val Loss: 1.4480\n","No improvement (3/3)\n","Early Stopping Triggered.\n","Training Finished.\n","Best model saved at: /content/drive/MyDrive/recipe_project/best_model.pt\n"]}],"execution_count":11},{"cell_type":"markdown","source":"# **Model Testing**","metadata":{"id":"A2hMgV7DkuA0"}},{"cell_type":"code","source":"# Load the Best Saved Model\nmodel = DualEncoder(embed_dim=256, freeze_cnn=False).to(device)\nmodel.load_state_dict(torch.load(\"/content/drive/MyDrive/recipe_project/best_model.pt\", map_location=device))\nmodel.eval()\nprint(\"Best model loaded for testing.\")\n\n\n# 1. COMPUTE EMBEDDINGS FOR TEST SET\nimage_embeddings = []\ntext_embeddings = []\ntest_recipe_ids = []\n\nprint(\"Computing embeddings for Test Set\")\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n\n        # Get embeddings\n        img_emb, txt_emb = model(images, input_ids, mask)\n\n        # Store them on CPU to save GPU memory\n        image_embeddings.append(img_emb.cpu())\n        text_embeddings.append(txt_emb.cpu())\n        test_recipe_ids.extend(batch['recipe_id'])\n\n# Concatenate all batches\nimage_embeddings = torch.cat(image_embeddings)\ntext_embeddings = torch.cat(text_embeddings)\n\nprint(f\"Test Embeddings Shape: {image_embeddings.shape}\")\n\n# ---------------------------------------------------------\n# 2. CALCULATE COSINE SIMILARITY MATRIX\n# ---------------------------------------------------------\n# Normalize (just to be safe, though model does it too)\nimage_embeddings = F.normalize(image_embeddings, p=2, dim=1)\ntext_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n\n# Similarity Matrix: [N_test x N_test]\n# row i, col j = similarity between Image_i and Text_j\nsimilarity_matrix = image_embeddings @ text_embeddings.T\n\n# similarity_matrix shape: [N_images, N_texts]\n\n\n# IMAGE → TEXT RETRIEVAL\n\n\ndef recall_at_k_i2t(similarity_matrix, k=1):\n    N = similarity_matrix.shape[0]\n    correct = 0\n\n    for i in range(N):\n        sims = similarity_matrix[i]\n        topk_idx = torch.topk(sims, k).indices.tolist()\n        if i in topk_idx:\n            correct += 1\n\n    return correct / N\n\n\ndef median_rank_i2t(similarity_matrix):\n    N = similarity_matrix.shape[0]\n    ranks = []\n\n    for i in range(N):\n        sims = similarity_matrix[i]\n        sorted_idx = torch.argsort(sims, descending=True)\n        rank = (sorted_idx == i).nonzero(as_tuple=False).item() + 1\n        ranks.append(rank)\n\n    return float(np.median(ranks))\n\n\n\n# TEXT → IMAGE RETRIEVAL\n\n\ndef recall_at_k_t2i(similarity_matrix, k=1):\n    sim_t2i = similarity_matrix.T\n    N = sim_t2i.shape[0]\n    correct = 0\n\n    for i in range(N):\n        sims = sim_t2i[i]\n        topk_idx = torch.topk(sims, k).indices.tolist()\n        if i in topk_idx:\n            correct += 1\n\n    return correct / N\n\n\ndef median_rank_t2i(similarity_matrix):\n    sim_t2i = similarity_matrix.T\n    N = sim_t2i.shape[0]\n    ranks = []\n\n    for i in range(N):\n        sims = sim_t2i[i]\n        sorted_idx = torch.argsort(sims, descending=True)\n        rank = (sorted_idx == i).nonzero(as_tuple=False).item() + 1\n        ranks.append(rank)\n\n    return float(np.median(ranks))\n\n\n\n# PRINT RESULTS\n\nprint(\"\\n==============================\")\nprint(\"IMAGE → TEXT RETRIEVAL RESULTS\")\nprint(\"==============================\")\n\nfor k in [1, 5, 10]:\n    print(f\"Recall@{k}: {recall_at_k_i2t(similarity_matrix, k):.4f}\")\n\nprint(f\"Median Rank (MedR): {median_rank_i2t(similarity_matrix):.2f}\")\n\n\nprint(\"\\n==============================\")\nprint(\"TEXT → IMAGE RETRIEVAL RESULTS\")\nprint(\"==============================\")\n\nfor k in [1, 5, 10]:\n    print(f\"Recall@{k}: {recall_at_k_t2i(similarity_matrix, k):.4f}\")\n\nprint(f\"Median Rank (MedR): {median_rank_t2i(similarity_matrix):.2f}\")\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQSUQfNgzt_H","outputId":"38c1ed17-3eaa-411a-d1d0-a2a9fe29dde4","executionInfo":{"status":"ok","timestamp":1764536921854,"user_tz":300,"elapsed":17925,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Best model loaded for testing.\n","Computing embeddings for Test Set\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 23/23 [00:14<00:00,  1.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Embeddings Shape: torch.Size([710, 256])\n","\n","==============================\n","IMAGE → TEXT RETRIEVAL RESULTS\n","==============================\n","Recall@1: 0.1155\n","Recall@5: 0.3268\n","Recall@10: 0.4577\n","Median Rank (MedR): 13.00\n","\n","==============================\n","TEXT → IMAGE RETRIEVAL RESULTS\n","==============================\n","Recall@1: 0.1380\n","Recall@5: 0.3479\n","Recall@10: 0.4634\n","Median Rank (MedR): 13.00\n"]}],"execution_count":12},{"cell_type":"markdown","source":"# **Nutrition (Dietary constraints + health goal) Optimization Layer**","metadata":{"id":"taIGiPAEk2QW"}},{"cell_type":"code","source":"import pandas as pd\nimport zipfile\nimport os\nimport re\n\n# ---------------------------------------------------------\n# 1. UNZIP NUTRITION DATA\n# ---------------------------------------------------------\nNUTRITION_ZIP_PATH = \"/content/drive/MyDrive/recipe_project/archive (2).zip\"\nNUTRITION_EXTRACT_DIR = \"/content/nutrition_data\"\n\nif not os.path.exists(NUTRITION_EXTRACT_DIR):\n    print(f\"Unzipping {NUTRITION_ZIP_PATH}\")\n    try:\n        with zipfile.ZipFile(NUTRITION_ZIP_PATH, 'r') as z:\n            z.extractall(NUTRITION_EXTRACT_DIR)\n        print(\"Extraction complete.\")\n    except Exception as e:\n        print(f\"Error unzipping: {e}\")\nelse:\n    print(\"Folder already exists, skipping unzip.\")\n\n# ---------------------------------------------------------\n# 2. FIND ALL DATA CSVs\n# ---------------------------------------------------------\ndata_csv_files = []\nfor root, dirs, files in os.walk(NUTRITION_EXTRACT_DIR):\n    for file in files:\n        if file.endswith(\".csv\") and \"FOOD-DATA\" in file and \"METADATA\" not in file:\n            data_csv_files.append(os.path.join(root, file))\n\nif not data_csv_files:\n    raise FileNotFoundError(\"No suitable CSV files found!\")\n\nprint(f\"Found {len(data_csv_files)} data files. Merging...\")\n\n# ---------------------------------------------------------\n# 3. LOAD & MERGE KAGGLE DATA (SMARTER)\n# ---------------------------------------------------------\ndef get_val(row, possible_cols):\n    \"\"\"Helper to find the first valid column that exists\"\"\"\n    for col in possible_cols:\n        if col in row:\n            return float(row[col])\n    return 0.0\n\ndef load_all_nutrition_files(file_list):\n    lookup = {}\n\n    for csv_path in file_list:\n        try:\n            df = pd.read_csv(csv_path)\n            # Normalize columns\n            df.columns = [c.strip().lower().replace(\" \", \"_\").replace(\".\", \"\") for c in df.columns]\n\n            # --- Iterate rows ---\n            for _, row in df.iterrows():\n                # Get raw name\n                raw_name = str(row.get('food', '')).lower()\n                # Clean name: \"Chicken, raw\" -> \"chicken raw\"\n                clean_name = ''.join(c for c in raw_name if c.isalnum() or c.isspace()).strip()\n\n                if not clean_name: continue\n\n                # Extract Macros using flexible column names\n                macros = {\n                    'calories': get_val(row, ['caloric_value', 'calories', 'energy_kcal']),\n                    'protein':  get_val(row, ['protein', 'protein_g']),\n                    'fat':      get_val(row, ['fat', 'total_lipid_fat']),\n                    'carbs':    get_val(row, ['carbohydrates', 'carbohydrate_by_difference']),\n                    'sugar':    get_val(row, ['sugars', 'sugars_total'])\n                }\n\n                # --- STRATEGY: SAVE MULTIPLE KEYS ---\n                # 1. Save the full name: \"chicken raw\"\n                lookup[clean_name] = macros\n\n                # 2. Save the FIRST word: \"chicken\"\n                # (This fixes your \"Matched 0/3\" error!)\n                first_word = clean_name.split()[0]\n                if len(first_word) > 2 and first_word not in lookup:\n                     lookup[first_word] = macros\n\n        except Exception as e:\n            print(f\"Error reading {csv_path}: {e}\")\n\n    return lookup\n\nnutrition_lookup = load_all_nutrition_files(data_csv_files)\nprint(f\"Successfully loaded {len(nutrition_lookup)} lookup keys.\")\n\n# ---------------------------------------------------------\n# 4. HEURISTIC INGREDIENT PARSER\n# ---------------------------------------------------------\n# ---------------------------------------------------------\n# IMPROVED INGREDIENT PARSER (REPLACE OLD calculate_approx_macros)\n# ---------------------------------------------------------\n\ndef calculate_approx_macros(ingredients_list):\n    \"\"\"\n    Improved version:\n    1. Multi-word matching (e.g., 'chicken breast', 'olive oil')\n    2. Whole-word matching using regex boundaries\n    3. Fallback to single-word matching\n    4. Avoids false positives (egg in eggplant)\n    \"\"\"\n\n    total = {'calories': 0.0, 'protein': 0.0, 'fat': 0.0, 'carbs': 0.0, 'sugar': 0.0}\n    matched_count = 0\n\n    # Pre-clean all nutrition keys for quick search\n    nutrition_keys = list(nutrition_lookup.keys())\n\n    for ingredient in ingredients_list:\n        ingredient_clean = ingredient.lower()\n\n        # -----------------------------------------------\n        # 1. MULTI-WORD MATCHING (strongest and most accurate)\n        # -----------------------------------------------\n        match_found = False\n\n        for food_key in nutrition_keys:\n            # multi-word exact phrase match\n            pattern = r\"\\b\" + re.escape(food_key) + r\"\\b\"\n            if re.search(pattern, ingredient_clean):\n                macros = nutrition_lookup[food_key]\n                for k in total:\n                    total[k] += macros[k]\n                matched_count += 1\n                match_found = True\n                break\n\n        if match_found:\n            continue  # skip token matching if phrase match worked\n\n        # -----------------------------------------------\n        # 2. SINGLE-WORD TOKEN MATCHING (fallback)\n        # -----------------------------------------------\n        # extract only alphabetical tokens\n        tokens = re.findall(r\"[a-zA-Z]+\", ingredient_clean)\n\n        for token in tokens:\n            if len(token) <= 2:\n                continue  # skip useless small words\n\n            # whole-word match\n            if token in nutrition_lookup:\n                macros = nutrition_lookup[token]\n                for k in total:\n                    total[k] += macros[k]\n                matched_count += 1\n                match_found = True\n                break\n\n    # -----------------------------------------------\n    # 3. AVERAGE OVER MATCHED INGREDIENTS\n    # -----------------------------------------------\n    if matched_count > 0:\n        for k in total:\n            total[k] = round(total[k] / matched_count, 2)\n    else:\n        # if nothing matched, return zeros\n        total = {k: 0.0 for k in total}\n\n    return total, matched_count","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbW7ZDLYP33k","outputId":"b89f80a4-5581-45b1-d5c8-1b52d607db86","executionInfo":{"status":"ok","timestamp":1764537007141,"user_tz":300,"elapsed":3012,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unzipping /content/drive/MyDrive/recipe_project/archive (2).zip\n","Extraction complete.\n","Found 5 data files. Merging...\n","Successfully loaded 3112 lookup keys.\n"]}],"execution_count":13},{"cell_type":"code","source":"# DIETARY FILTER\n\nimport re\n\ndef check_dietary_compliance(ingredients_list, diet_filter, title=\"\"):\n    \"\"\"\n    Enhanced dietary filter:\n    - Case-insensitive\n    - Whole-word boundaries (\\bX\\b)\n    - Multi-word detection\n    - Checks BOTH ingredients + title\n    \"\"\"\n\n    diet = diet_filter.lower().strip()\n\n    # Combine ingredients and title into a unified text blob\n    text_blob = (title + \" \" + \" \".join(ingredients_list)).lower()\n\n    # FOOD CATEGORY LISTS\n\n    MEAT = [\n        \"chicken\", \"chicken breast\", \"ground chicken\",\n        \"beef\", \"ground beef\", \"steak\", \"veal\",\n        \"mutton\", \"lamb\", \"goat\",\n        \"pork\", \"bacon\", \"ham\", \"pepperoni\", \"salami\", \"sirloin\", \"tenderloin\"\n        \"sausage\", \"turkey\", \"ground chuck\", \"chuck\", \"hen\"\n    ]\n\n    FISH = [\n        \"fish\", \"seafood\", \"mixed seafood\", \"frozen seafood\",\n        \"seafood mix\", \"seafood medley\",\n        \"salmon\", \"tuna\", \"shrimp\", \"prawn\", \"anchovy\", \"cod\",\n        \"crab\", \"lobster\", \"scallop\", \"mussel\", \"clam\", \"oyster\",\n        \"squid\", \"octopus\", \"mackerel\", \"trout\", \"snapper\", \"scallops\", \"clams\"\n    ]\n\n    DAIRY = [\n        \"milk\", \"cheese\", \"butter\", \"yogurt\",\n        \"cream\", \"parmesan\", \"paneer\", \"ghee\", \"whey\"\n    ]\n\n    EGGS = [\"egg\", \"egg yolk\", \"eggs\"]\n\n    HONEY = [\"honey\"]\n\n    PORK = [\"pork\", \"bacon\", \"ham\", \"salami\", \"pepperoni\", \"lard\", \"chorizo\"]\n\n    ALCOHOL = [\"wine\", \"vodka\", \"rum\", \"beer\", \"whiskey\", \"bourbon\", \"liqueur\"]\n\n    GLUTEN = [\"wheat\", \"flour\", \"bread\", \"noodles\", \"pasta\", \"spaghetti\", \"barley\", \"rye\", \"soy sauce\"]\n\n    # Helpers for whole-word matching\n    def contains(words):\n        for w in words:\n            pattern = r\"\\b\" + re.escape(w) + r\"\\b\"\n            if re.search(pattern, text_blob):\n                return True\n        return False\n\n    # -----------------------------\n    # APPLY DIET RULES\n    # -----------------------------\n\n    # Vegan: no meat, fish, dairy, eggs, honey\n    if diet == \"vegan\":\n        if contains(MEAT+FISH+DAIRY+EGGS+HONEY):\n            return False\n\n    # Vegetarian: no meat, no fish\n    if diet == \"vegetarian\":\n        if contains(MEAT+FISH):\n            return False\n\n    # Pescatarian: no meat, fish allowed\n    if diet == \"pescatarian\":\n        if contains(MEAT):\n            return False\n\n    # Halal: no pork, no alcohol\n    if diet == \"halal\":\n        if contains(PORK+ALCOHOL):\n            return False\n\n    # Gluten free\n    if diet == \"gluten free\":\n        if contains(GLUTEN):\n            return False\n\n    return True\n","metadata":{"id":"T72rZriLQLmy","executionInfo":{"status":"ok","timestamp":1764537014616,"user_tz":300,"elapsed":9,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ---------------------------------------------------------\n# BUILD FULL DATABASE INDEX (Train + Val + Test)\n# ---------------------------------------------------------\n\n# 1. Prepare Data\nall_recipe_ids = [item['recipe_id'] for item in full_data]\nfull_db_embeddings = []\n\n# 2. Create a DataLoader for the Full Dataset (Text Only)\n# We can reuse the RecipeDataset class but we only need text\nfull_dataset = RecipeDataset(full_data, FINAL_IMG_DIR, val_transform, tokenizer)\nfull_loader = DataLoader(full_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# 3. Compute Embeddings\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(full_loader, desc=\"Indexing Full DB\"):\n        input_ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n\n        # Get Text Embeddings from the Model\n        # We manually call the text branch since we don't have images here\n        txt_out = model.text_encoder(input_ids=input_ids, attention_mask=mask)\n        txt_feat = txt_out.last_hidden_state[:, 0, :]\n        txt_feat = model.text_proj(txt_feat)\n        txt_feat = F.normalize(txt_feat, p=2, dim=1)\n\n        full_db_embeddings.append(txt_feat.cpu())\n\n# 4. Concatenate and move to device once, making it a global variable\nglobal db_emb\ndb_emb = torch.cat(full_db_embeddings).to(device)\nprint(f\"Full Database Indexed. Shape: {db_emb.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V--r92EnQdcY","outputId":"33a6285f-68ae-407e-d0b1-ae8cb334e4ae","executionInfo":{"status":"ok","timestamp":1764537114931,"user_tz":300,"elapsed":96834,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Indexing Full DB: 100%|██████████| 111/111 [01:36<00:00,  1.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Full Database Indexed. Shape: torch.Size([7100, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"execution_count":15},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom PIL import Image as PILImage\n\n# 0. SETUP\nif 'id_to_entry' not in globals():\n    id_to_entry = {item['recipe_id']: item for item in full_data}\n\n\n# 1. RETRIEVAL FUNCTION\ndef retrieve_recipe_from_image(image_path, topk=50):\n    if topk is None: topk = 50\n    try:\n        img = PILImage.open(image_path).convert(\"RGB\")\n        img_t = val_transform(img).unsqueeze(0).to(device)\n    except:\n        return []\n\n    with torch.no_grad():\n        img_emb, _ = model(images=img_t)\n        img_emb = F.normalize(img_emb, p=2, dim=1)\n\n    sims = (img_emb @ db_emb.T).squeeze(0)\n\n    max_k = min(int(topk), len(all_recipe_ids))\n    top_indices = torch.topk(sims, k=max_k).indices.tolist()\n\n    results = []\n    for idx in top_indices:\n        rid = all_recipe_ids[idx]\n        entry = id_to_entry.get(rid)\n        if entry:\n            results.append({\n                \"recipe_id\": rid,\n                \"similarity\": sims[idx].item(),\n                \"image_filename\": entry['image_filename'],\n                \"title\": entry['title']\n            })\n    return results","metadata":{"id":"KgD0hlfQQgbd","executionInfo":{"status":"ok","timestamp":1764537124630,"user_tz":300,"elapsed":4,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# NUTRITION SCORING\n\n# Convert nutrition lookup (all individual foods) into a DataFrame\nall_macros = pd.DataFrame(nutrition_lookup.values())\n\n# Fix column names\nall_macros = all_macros.rename(columns={\n    \"sugar\": \"sugar\"\n})\n\nmax_cal = all_macros[\"calories\"].max()\nmax_pro = all_macros[\"protein\"].max()\nmax_fat = all_macros[\"fat\"].max()\nmax_carb = all_macros[\"carbs\"].max()\nmax_sug = all_macros[\"sugar\"].max()\n\nprint(\"Max macro values computed.\")\n\ndef nutrition_objective_score(macros, objective):\n    if macros is None:\n        return 0.0\n\n    cal  = macros.get(\"calories\", 0)\n    pro  = macros.get(\"protein\", 0)\n    fat  = macros.get(\"fat\", 0)\n    carb = macros.get(\"carbs\", 0)\n    sug  = macros.get(\"sugar\", 0)\n\n    # Normalization\n    cal_n  = cal  / (max_cal  + 1e-6)\n    pro_n  = pro  / (max_pro  + 1e-6)\n    fat_n  = fat  / (max_fat  + 1e-6)\n    carb_n = carb / (max_carb + 1e-6)\n    sug_n  = sug  / (max_sug  + 1e-6)\n\n    # Objective scoring\n    if objective == \"high_protein\":\n        return pro_n\n\n    if objective == \"low_calorie\":\n        return 1.0 - cal_n\n\n    if objective == \"low_fat\":\n        return 1.0 - fat_n\n\n    if objective == \"low_carb\":\n        return 1.0 - carb_n\n\n    if objective == \"sugar_free\":\n        return 1.0 - sug_n\n\n    if objective == \"keto\":\n        return 0.7*(1-carb_n) + 0.3*(fat_n)\n\n    return 0.0\n","metadata":{"id":"ZdYeFYwxSawq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05e5b4e6-ddc0-4589-cdb2-e7bf2a8e7691","executionInfo":{"status":"ok","timestamp":1764537127605,"user_tz":300,"elapsed":4,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Max macro values computed.\n"]}],"execution_count":17},{"cell_type":"code","source":"# 2. RE-RANKING FUNCTION\n\ndef rerank_with_constraints(results, diet_type=\"none\", objective=\"none\", alpha=0.5):\n    final_list = []\n\n    # sanitize inputs\n    diet_type = diet_type.lower()\n    objective = objective.lower()\n\n    for item in results:\n        rid = item[\"recipe_id\"]\n        entry = id_to_entry.get(rid)\n        if entry is None:\n            continue\n\n        # Compute macros\n        macros, _ = calculate_approx_macros(entry['ingredients'])\n\n        # Apply diet filter\n        if diet_type != \"none\":\n            if not check_dietary_compliance(entry['ingredients'], diet_type, entry['title']):\n                continue\n\n        # Nutrition score\n        nut_score = nutrition_objective_score(macros, objective)\n\n        # Visual similarity\n        sim = float(item[\"similarity\"])\n\n        # Final score (balanced)\n        final_score = (alpha * nut_score) + ((1 - alpha) * sim)\n\n        enriched = {\n            \"title\": entry.get(\"title\", \"Unknown\"),\n            \"image_filename\": entry.get(\"image_filename\", \"\"),\n            \"ingredients\": entry.get(\"ingredients\", []),\n            \"instructions\": entry.get(\"instructions\", []),\n            \"final_score\": final_score,\n            \"macros\": macros,\n            \"similarity\": sim\n        }\n        final_list.append(enriched)\n\n    # Sort descending\n    final_list.sort(key=lambda x: x[\"final_score\"], reverse=True)\n    return final_list\n","metadata":{"id":"79X3rmhZVNit","executionInfo":{"status":"ok","timestamp":1764537129993,"user_tz":300,"elapsed":109,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# MANUAL TESTING\n\nimport os\n\ntest_image_path = os.path.join(PROJECT_ROOT, \"creamy-mushroom-pasta-recipe-8.jpg\")\nprint(f\"Query Image Path: {test_image_path}\")\n\nprint(\"Step 1: Retrieving Top 10 visual matches...\")\nraw_results = retrieve_recipe_from_image(test_image_path, topk=200)\n\ndiet = \"vegetarian\"\ngoal = \"low_calorie\"\n\nprint(f\"Step 2: Re-ranking with Diet='{diet}' and Goal='{goal}'...\")\n\nranked_results = rerank_with_constraints(\n    raw_results,\n    diet_type=diet,\n    objective=goal,\n    alpha=0.7\n)\n\nprint(\"\\n\" + \"=\"*90)\nprint(f\"TOP 10 RESULTS FOR '{goal.upper()}'\")\nprint(\"=\"*90)\n\nprint(f\"{'RANK':<5} | {'SCORE':<8} | {'CAL':<6} | {'PRO':<6} | {'FAT':<6} | {'CARB':<6} | {'SUG':<6} | TITLE\")\nprint(\"-\" * 90)\n\nfor i, r in enumerate(ranked_results[:10]):\n    cal = r[\"macros\"][\"calories\"]\n    pro = r[\"macros\"][\"protein\"]\n    fat = r[\"macros\"][\"fat\"]\n    carb = r[\"macros\"][\"carbs\"]\n    sug = r[\"macros\"][\"sugar\"]\n\n    print(\n        f\"#{i+1:<4} | {r['final_score']:.4f} | \"\n        f\"{cal:<6.1f} | {pro:<6.1f} | {fat:<6.1f} | {carb:<6.1f} | {sug:<6.1f} | \"\n        f\"{r['title'][:40]}\"\n    )\n\nprint(\"=\"*90)\nprint(\"ℹNOTE: Final Score = α * NutritionScore + (1-α) * VisualSimilarity\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saovDbfaUATp","outputId":"d5987211-d745-4312-e450-93c06900de65","executionInfo":{"status":"ok","timestamp":1764537294150,"user_tz":300,"elapsed":52585,"user":{"displayName":"Tushar Bhatia","userId":"07046111014283208839"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Query Image Path: /content/recipe_project_12k/creamy-mushroom-pasta-recipe-8.jpg\n","Step 1: Retrieving Top 10 visual matches...\n","Step 2: Re-ranking with Diet='vegetarian' and Goal='low_calorie'...\n","\n","==========================================================================================\n","TOP 10 RESULTS FOR 'LOW_CALORIE'\n","==========================================================================================\n","RANK  | SCORE    | CAL    | PRO    | FAT    | CARB   | SUG    | TITLE\n","------------------------------------------------------------------------------------------\n","#1    | 0.8882 | 56.1   | 1.9    | 3.0    | 5.3    | 0.4    | Spinach and Sausage Pasta\n","#2    | 0.8874 | 129.8  | 3.6    | 11.1   | 5.3    | 0.6    | Sesame Peanut Noodles\n","#3    | 0.8828 | 56.5   | 0.7    | 4.2    | 4.0    | 0.3    | The Best Garlic & Chives Noodles\n","#4    | 0.8823 | 81.0   | 2.1    | 2.3    | 11.5   | 0.9    | Winter Herb Pasta\n","#5    | 0.8816 | 94.5   | 1.2    | 3.6    | 14.8   | 1.6    | Taiwanese Cold Sesame Noodles\n","#6    | 0.8792 | 91.2   | 0.9    | 4.2    | 12.6   | 1.1    | Spicy Broccoli and Soba Noodle Stir-Fry\n","#7    | 0.8759 | 65.4   | 1.1    | 3.9    | 7.0    | 2.1    | Panda Express Chow Mein\n","#8    | 0.8590 | 100.3  | 2.5    | 4.5    | 12.7   | 4.4    | Spaghetti With Sausage-Mushroom Sauce\n","#9    | 0.8560 | 88.2   | 1.1    | 4.5    | 11.4   | 2.5    | A banana to share\n","#10   | 0.8510 | 64.4   | 0.6    | 5.1    | 3.1    | 0.8    | Noodles With Spicy Peanut Sauce\n","==========================================================================================\n","ℹNOTE: Final Score = α * NutritionScore + (1-α) * VisualSimilarity\n"]}],"execution_count":20},{"cell_type":"code","source":"# UI-Based Interactive Test\n\nimport gradio as gr\nimport torch.nn.functional as F\nfrom PIL import Image as PILImage\nimport numpy as np\nimport os\n\n# 3. FORMATTING RESULT\n\ndef format_multi_result(recipes):\n    output_text = \"\"\n    for i, r in enumerate(recipes[:3]):\n        m = r['macros']\n\n        # Header\n        output_text += f\"## Result {i+1}: {r['title']}\\n\\n\"\n\n        # Stats Block\n        output_text += f\"**Score:** {r['final_score']:.2f}\\n\\n\"\n        output_text += f\"**Nutrition (100g):** {m['calories']} cal | **{m['protein']}g Pro** | {m['fat']}g Fat | {m['carbs']}g Carb\\n\\n\"\n\n        # Ingredients List\n        output_text += \"### 🥣 Ingredients\\n\"\n        # Join with newlines explicitly\n        ing_formatted = \"\\n\".join([f\"- {ing}\" for ing in r['ingredients'][:8]])\n        output_text += f\"{ing_formatted}\\n\"\n\n        if len(r['ingredients']) > 8:\n            output_text += f\"- ... ({len(r['ingredients']) - 8} more)\\n\"\n\n        # Instructions List\n        output_text += \"\\n### 📝 Instructions\\n\"\n        inst_formatted = \"\\n\".join([f\"{j+1}. {inst}\" for j, inst in enumerate(r['instructions'][:5])])\n        output_text += f\"{inst_formatted}\\n\"\n\n        if len(r['instructions']) > 5:\n            output_text += f\"- ... ({len(r['instructions']) - 5} more steps)\\n\"\n\n        output_text += \"\\n---\\n\\n\"\n    return output_text\n\n# GRADIO MAIN\ndef gradio_retrieve(img, diet_type, objective, topk):\n    if img is None: return \"Please upload an image.\", None\n\n    img.save(\"/content/query_image.jpg\")\n    raw = retrieve_recipe_from_image(\"/content/query_image.jpg\", topk=topk)\n    final = rerank_with_constraints(raw, diet_type, objective, alpha=0.3)\n\n    if not final:\n        return \"No recipes found.\", None\n\n    text_block = format_multi_result(final)\n\n    gallery_data = []\n    for r in final[:3]:\n        img_path = os.path.join(FINAL_IMG_DIR, r[\"image_filename\"])\n        if os.path.exists(img_path):\n            pil_img = PILImage.open(img_path).convert(\"RGB\")\n            gallery_data.append((pil_img, r[\"title\"])) # Completed the append statement\n\n    return text_block, gallery_data # Added the return statement","metadata":{"id":"lnHcC5ptUCa_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport torch.nn.functional as F\nfrom PIL import Image as PILImage\nimport numpy as np\nimport os\n\n# ---------------------------------------------------------\n# FORMATTER\n# ---------------------------------------------------------\ndef format_multi_result(recipes, topk=3):\n    output_text = \"\"\n    for i, r in enumerate(recipes[:topk]):\n        m = r['macros']\n\n        output_text += f\"## Result {i+1}: {r['title']}\\n\\n\"\n\n        output_text += f\"**Score:** {r['final_score']:.3f}\\n\\n\"\n        output_text += (\n            f\"**Estimated Nutrition:** {m['calories']} cal | \"\n            f\"**{m['protein']}g Protein** | {m['fat']}g Fat | {m['carbs']}g Carbs\\n\\n\"\n        )\n\n        output_text += \"### 🥣 Ingredients\\n\"\n        ing_formatted = \"\\n\".join([f\"- {ing}\" for ing in r['ingredients'][:8]])\n        output_text += ing_formatted + \"\\n\"\n        if len(r['ingredients']) > 8:\n            output_text += f\"- ... ({len(r['ingredients']) - 8} more)\\n\"\n\n        output_text += \"\\n### 📝 Instructions\\n\"\n        inst_formatted = \"\\n\".join([f\"{j+1}. {inst}\"\n                                     for j, inst in enumerate(r['instructions'][:5])])\n        output_text += inst_formatted + \"\\n\"\n        if len(r['instructions']) > 5:\n            output_text += f\"- ... ({len(r['instructions']) - 5} more steps)\\n\"\n\n        output_text += \"\\n---\\n\\n\"\n    return output_text\n\n\n# ---------------------------------------------------------\n# MAIN GRADIO FUNCTION\n# ---------------------------------------------------------\ndef gradio_retrieve(img, diet_type, objective, topk):\n    if img is None:\n        return \"Please upload an image.\", None\n\n    img_path = \"/content/query_image.jpg\"\n    img.save(img_path)\n\n    raw = retrieve_recipe_from_image(img_path, topk=topk)\n\n    final = rerank_with_constraints(\n        raw,\n        diet_type=diet_type.lower().strip(),   # ★ FIX 1\n        objective=objective.lower().strip(),   # ★ FIX 2\n        alpha=0.3\n    )\n\n    if not final:\n        return \"No recipes found.\", None\n\n    text_block = format_multi_result(final, topk=3)\n\n    gallery_data = []\n    for r in final[:3]:\n        img_path = os.path.join(FINAL_IMG_DIR, r[\"image_filename\"])\n        if os.path.exists(img_path):\n            gallery_data.append((PILImage.open(img_path), r[\"title\"]))\n\n    return text_block, gallery_data\n\n\n# ---------------------------------------------------------\n# GRADIO UI\n# ---------------------------------------------------------\nif __name__ == '__main__':\n\n    diet_opts = ['none','vegan','vegetarian','pescatarian','halal','gluten free']  # ★ FIX 3\n\n    goal_opts = ['none','high_protein','low_calorie','low_fat','low_carb',\n                 'sugar_free','keto']  # ★ FIX 4\n\n    image_input = gr.Image(type=\"pil\", label=\"Upload an image\")\n\n    diet_dropdown = gr.Dropdown(choices=diet_opts, value='none',\n                                label=\"Dietary Preference\")\n\n    objective_dropdown = gr.Dropdown(choices=goal_opts, value='none',\n                                     label=\"Nutritional Goal\")\n\n    topk_slider = gr.Slider(minimum=5, maximum=200, value=50, step=5,\n                             label=\"Retrieve Top-K Candidates\")  # ★ FIX 5\n\n    text_output = gr.Markdown(label=\"Recipe Details\")\n    gallery_output = gr.Gallery(label=\"Recipe Images\")\n\n    demo = gr.Interface(\n        fn=gradio_retrieve,\n        inputs=[image_input, diet_dropdown, objective_dropdown, topk_slider],\n        outputs=[text_output, gallery_output],\n        title=\"Recipe Retrieval (Image → Recipe + Nutrition + Diet Filters)\",\n        description=\"Upload a dish image and get matching recipes filtered by diet and nutritional goals.\"\n    )\n\n    demo.launch(debug=True)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"c0f0d28f","outputId":"de73923a-ba5b-4c92-f2c4-a574ee77e6d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://ff49782d92025ef90b.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ff49782d92025ef90b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["ERROR:    Exception in ASGI application\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n","    result = await app(  # type: ignore[func-returns-value]\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n","    return await self.app(scope, receive, send)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n","    await super().__call__(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n","    await self.middleware_stack(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n","    raise exc\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n","    await self.app(scope, receive, _send)\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n","    return await self.app(scope, receive, send)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n","    await self.app(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n","    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n","    raise exc\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n","    await app(scope, receive, sender)\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n","    await self.app(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n","    await self.middleware_stack(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n","    await route.handle(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n","    await self.app(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n","    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n","    raise exc\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n","    await app(scope, receive, sender)\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n","    response = await f(request)\n","               ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n","    raw_response = await run_endpoint_function(\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n","    return await dependant.call(**values)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n","    await asyncio.wait_for(\n","  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n","    return await fut\n","           ^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n","    return await self._signals[upload_id].wait()\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n","    fut = self._get_loop().create_future()\n","          ^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n","    raise RuntimeError(f'{self!r} is bound to a different event loop')\n","RuntimeError: <asyncio.locks.Event object at 0x7db231fe68a0 [unset]> is bound to a different event loop\n"]}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"g3FR1wbvV_QS"},"outputs":[],"execution_count":null}]}