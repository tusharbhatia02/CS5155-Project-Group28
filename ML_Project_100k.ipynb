{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8820139,"sourceType":"datasetVersion","datasetId":5306169},{"sourceId":13870065,"sourceType":"datasetVersion","datasetId":8837207},{"sourceId":13904434,"sourceType":"datasetVersion","datasetId":8858804},{"sourceId":13915740,"sourceType":"datasetVersion","datasetId":8866856},{"sourceId":13915900,"sourceType":"datasetVersion","datasetId":8866980},{"sourceId":13916069,"sourceType":"datasetVersion","datasetId":8867098},{"sourceId":13938033,"sourceType":"datasetVersion","datasetId":8882635}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n!pip install -q transformers tqdm scikit-learn pillow ijson pandas gradio","metadata":{"id":"m5z4EWgT223t","outputId":"1cf48f7b-acb1-4d58-9ed8-3b7b6375fa16","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:31:51.914550Z","iopub.execute_input":"2025-12-01T00:31:51.914832Z","iopub.status.idle":"2025-12-01T00:32:00.367479Z","shell.execute_reply.started":"2025-12-01T00:31:51.914804Z","shell.execute_reply":"2025-12-01T00:32:00.366586Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.4/134.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\n\nimport os\nimport json\nimport ijson\nimport zipfile\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn as nn\n\n\n# Detect GPUs\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    print(f\"GPUs available: {num_gpus}\")\n\n    if num_gpus > 1:\n        device = torch.device(\"cuda\")\n        multi_gpu = True\n        print(\"Using MULTI-GPU (DataParallel).\")\n    else:\n        device = torch.device(\"cuda:0\")\n        multi_gpu = False\n        print(\"Using single GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    multi_gpu = False\n    print(\"Using CPU.\")\n\n\n# Path Configuration\n\n\n# Path to the extracted image folder\nPROJECT_ROOT = \"/kaggle/input/mineeee2/recipe1m_tiny_100k\"\n\nFINAL_IMG_DIR = \"/kaggle/input/mineeee2/recipe1m_tiny_100k\"\n\n# JSON files\nLAYER1_JSON = \"/kaggle/input/mineee/layer1.json\"\nLAYER2_JSON = \"/kaggle/input/mineee/layer2.json\"\n\nIMAGES_DIR = FINAL_IMG_DIR  # For consistency\nprint(\"Configuration complete.\")\n\n# os.makedirs(IMAGES_DIR, exist_ok=True)\n\nprint(\"Configuration complete.\")","metadata":{"id":"JHn3Y8cBAE7p","outputId":"eec56318-cea3-40e5-8c7a-3c0819b69910","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:33:23.556340Z","iopub.execute_input":"2025-12-01T00:33:23.556635Z","iopub.status.idle":"2025-12-01T00:33:34.547911Z","shell.execute_reply.started":"2025-12-01T00:33:23.556604Z","shell.execute_reply":"2025-12-01T00:33:34.547059Z"}},"outputs":[{"name":"stdout","text":"GPUs available: 2\nUsing MULTI-GPU (DataParallel).\nConfiguration complete.\nConfiguration complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Data Preparation**","metadata":{"id":"Pg8zdTnEAxX6"}},{"cell_type":"code","source":"def unzip_data(zip_path, extract_to):\n\n    # Check if we already unzipped (saves time on re-runs)\n    if len(os.listdir(extract_to)) > 100:\n        print(f\"Folder {extract_to} already has files. Skipping unzip.\")\n        return\n\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        # Filter to extract only JPGs if zip is messy\n        for file in tqdm(zip_ref.namelist(), desc=\"Extracting\"):\n            if file.endswith(('.jpg', '.jpeg', '.png')):\n                zip_ref.extract(file, extract_to)\n\n    print(\"Extraction complete.\")\n\n# Run the extraction\nunzip_data(\"/kaggle/input/mineeee2/recipe1m_tiny_100k\", IMAGES_DIR)\n\n# Handle nested folders (if zip created a subfolder)\ncontents = os.listdir(IMAGES_DIR)\n# If the zip created a folder like \"recipe1m_12k/\" inside \"images/\", detect it\nif len(contents) == 1 and os.path.isdir(os.path.join(IMAGES_DIR, contents[0])):\n    FINAL_IMG_DIR = os.path.join(IMAGES_DIR, contents[0])\nelse:\n    FINAL_IMG_DIR = IMAGES_DIR\n\nprint(f\"Images are located in: {FINAL_IMG_DIR}\")\nprint(f\"Image Count: {len([f for f in os.listdir(FINAL_IMG_DIR) if f.endswith('.jpg')])}\")","metadata":{"id":"JGU8qOZPAl90","outputId":"af5a87e6-9200-4de2-f87e-f08da99246d3","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:33:49.802897Z","iopub.execute_input":"2025-12-01T00:33:49.803442Z","iopub.status.idle":"2025-12-01T00:33:51.052190Z","shell.execute_reply.started":"2025-12-01T00:33:49.803416Z","shell.execute_reply":"2025-12-01T00:33:51.051419Z"}},"outputs":[{"name":"stdout","text":"Folder /kaggle/input/mineeee2/recipe1m_tiny_100k already has files. Skipping unzip.\nImages are located in: /kaggle/input/mineeee2/recipe1m_tiny_100k\nImage Count: 100000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Matching Images to Text","metadata":{"id":"bDywq3WMEEtN"}},{"cell_type":"code","source":"# Get set of available image filenames on disk\navailable_images = set(os.listdir(FINAL_IMG_DIR))\nprint(f\"Found {len(available_images)} images on disk.\")\n\n# 1. Map Image Filename -> Recipe ID (using layer2.json)\nmatched_recipe_ids = set()\nid_to_img = {}\n\nprint(\"Streaming layer2+.json to match images\")\nwith open(LAYER2_JSON, 'rb') as f:\n\n    for entry in ijson.items(f, 'item'):\n        rid = entry['id']\n        for img in entry.get('images', []):\n            fname = img['id']\n            # If this image exists in our 12k folder\n            if fname in available_images:\n                matched_recipe_ids.add(rid)\n                id_to_img[rid] = fname\n                break\n\nprint(f\"Matched {len(matched_recipe_ids)} Recipe IDs.\")\n\n# 2. Extract Text for those IDs (using layer1.json)\nfinal_dataset = []\n\nprint(\"Streaming layer1.json to extract text\")\nwith open(LAYER1_JSON, 'rb') as f:\n    \n    for entry in ijson.items(f, 'item'):\n        rid = entry['id']\n        if rid in matched_recipe_ids:\n            final_dataset.append({\n                \"recipe_id\": rid,\n                \"image_filename\": id_to_img[rid],\n                \"title\": entry.get(\"title\", \"Untitled\"),\n                \"ingredients\": [x['text'] for x in entry.get(\"ingredients\", [])],\n                \"instructions\": [x['text'] for x in entry.get(\"instructions\", [])]\n            })\n\nprint(f\"Final Dataset Size: {len(final_dataset)} pairs.\")","metadata":{"id":"YMSqqEv7BGck","outputId":"86e18a27-12b0-48d4-b0aa-1a0426535db6","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:33:58.496928Z","iopub.execute_input":"2025-12-01T00:33:58.497266Z","iopub.status.idle":"2025-12-01T00:34:20.201803Z","shell.execute_reply.started":"2025-12-01T00:33:58.497233Z","shell.execute_reply":"2025-12-01T00:34:20.200980Z"}},"outputs":[{"name":"stdout","text":"Found 100000 images on disk.\nStreaming layer2+.json to match images\nMatched 48982 Recipe IDs.\nStreaming layer1.json to extract text\nFinal Dataset Size: 48982 pairs.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Save cleaned dataset in Kaggle Working directory\nOUTPUT_PATH = \"/kaggle/working/dataset_100k_clean.json\"\n\nwith open(OUTPUT_PATH, \"w\") as f:\n    json.dump(final_dataset, f)\n\nprint(f\"Dataset saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:34:23.613976Z","iopub.execute_input":"2025-12-01T00:34:23.614693Z","iopub.status.idle":"2025-12-01T00:34:24.581552Z","shell.execute_reply.started":"2025-12-01T00:34:23.614657Z","shell.execute_reply":"2025-12-01T00:34:24.580733Z"}},"outputs":[{"name":"stdout","text":"Dataset saved to: /kaggle/working/dataset_100k_clean.json\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# A. AUGMENTATION\n# We use augmentation for training to force the model to learn features, not pixels.\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),        # Resize slightly larger than target\n    transforms.RandomCrop(224),           # Randomly crop the 224x224 area\n    transforms.RandomHorizontalFlip(),    # Flip left/right (food looks same flipped)\n    transforms.RandomRotation(15),        # Rotate +/- 15 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Vary lighting\n    transforms.ToTensor(),                # Convert to Tensor\n    transforms.Normalize(                 # Normalize to ImageNet standards\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# For Validation, resize and normalize (No randomness)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\n# B. DATASET CLASS\n\nclass RecipeDataset(Dataset):\n    def __init__(self, data_list, img_dir, transform, tokenizer, max_len=128):\n        self.data = data_list\n        self.img_dir = img_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        # 1. Image\n        img_path = os.path.join(self.img_dir, item['image_filename'])\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n            image = self.transform(image)\n        except:\n            # Fallback for corrupted images\n            image = torch.zeros(3, 224, 224)\n\n        # 2. Text (Concatenate Title + Ingredients + Instructions)\n        # We join them to give BERT maximum context\n        text_input = f\"{item['title']} [SEP] \" + \\\n                     \" \".join(item['ingredients']) + \\\n                     \" [SEP] \" + \\\n                     \" \".join(item['instructions'])\n\n        encoded = self.tokenizer(\n            text_input,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        return {\n            \"image\": image,\n            \"input_ids\": encoded['input_ids'].squeeze(0),\n            \"attention_mask\": encoded['attention_mask'].squeeze(0),\n            \"recipe_id\": item['recipe_id'] # Keep ID for lookup later\n        }\n\nprint(\"Transforms and Dataset class defined.\")","metadata":{"id":"T94uO_uKEnh9","outputId":"6f687ee8-34fd-49a9-ca2b-7b6ee46a1f67","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:34:33.543156Z","iopub.execute_input":"2025-12-01T00:34:33.543949Z","iopub.status.idle":"2025-12-01T00:34:33.558991Z","shell.execute_reply.started":"2025-12-01T00:34:33.543911Z","shell.execute_reply":"2025-12-01T00:34:33.558120Z"}},"outputs":[{"name":"stdout","text":"Transforms and Dataset class defined.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Dataset Split and Loaders\nDATASET_PATH = \"/kaggle/working/dataset_100k_clean.json\"\n\nwith open(DATASET_PATH, \"r\") as f:\n    full_data = json.load(f)\n\nprint(f\"Loaded {len(full_data)} (image, text) pairs from {DATASET_PATH}\")\n\n\n# 2. 3-WAY SPLIT: Train (80%), Validation (10%), Test (10%)\n\nfrom sklearn.model_selection import train_test_split\n\n# To Ensure reproducibility\nRANDOM_SEED = 42\n\n# Shuffle entire dataset once\nnp.random.seed(RANDOM_SEED)\nnp.random.shuffle(full_data)\n\n# Train / Temp split (80% / 20%)\ntrain_list, temp_list = train_test_split(\n    full_data,\n    test_size=0.20,\n    random_state=RANDOM_SEED\n)\n\n# Temp → Val/Test split (50% / 50% of the 20%)\nval_list, test_list = train_test_split(\n    temp_list,\n    test_size=0.50,\n    random_state=RANDOM_SEED\n)\n\nprint(\"Dataset Splits:\")\nprint(f\" Train      : {len(train_list)} ({len(train_list)/len(full_data)*100:.1f}%)\")\nprint(f\" Validation : {len(val_list)} ({len(val_list)/len(full_data)*100:.1f}%)\")\nprint(f\" Test       : {len(test_list)} ({len(test_list)/len(full_data)*100:.1f}%)\")\n\n\n\n# 3. TOKENIZER (DistilBERT)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"distilbert-base-uncased\",\n    model_max_length=128,\n    padding=\"max_length\",\n    truncation=True\n)\n\nprint(\"Tokenizer initialized.\")\n\n\n\n# 4. DATASETS\n\ntrain_dataset = RecipeDataset(\n    train_list, FINAL_IMG_DIR, train_transform, tokenizer\n)\nval_dataset = RecipeDataset(\n    val_list, FINAL_IMG_DIR, val_transform, tokenizer\n)\ntest_dataset = RecipeDataset(\n    test_list, FINAL_IMG_DIR, val_transform, tokenizer\n)\n\nprint(\"Datasets constructed.\")\n\n\n# 5. DATA LOADERS\n\nBATCH_SIZE = 64\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=2, pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\nprint(f\"DataLoaders ready (batch_size={BATCH_SIZE}).\")","metadata":{"id":"dsQUdjRiFmYV","outputId":"bf1833f9-eecd-4b57-af62-d38118fa42cd","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:39:44.312066Z","iopub.execute_input":"2025-12-01T00:39:44.312500Z","iopub.status.idle":"2025-12-01T00:39:45.989842Z","shell.execute_reply.started":"2025-12-01T00:39:44.312467Z","shell.execute_reply":"2025-12-01T00:39:45.989177Z"}},"outputs":[{"name":"stdout","text":"Loaded 48982 (image, text) pairs from /kaggle/working/dataset_100k_clean.json\nDataset Splits:\n Train      : 39185 (80.0%)\n Validation : 4898 (10.0%)\n Test       : 4899 (10.0%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb9daa7e305e463faf5195a264a908b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac68acf3e8e4d79912f2de0b7836046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5360a0dbab754c6b8a2ed809829d5dcd"}},"metadata":{}},{"name":"stdout","text":"Tokenizer initialized.\nDatasets constructed.\nDataLoaders ready (batch_size=64).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# **Model Architecture and Training**","metadata":{"id":"K7GXiISdGxmk"}},{"cell_type":"code","source":"# ---------------------------------------------------------\n# DUAL ENCODER\n# ---------------------------------------------------------\nclass DualEncoder(nn.Module):\n    def __init__(self, embed_dim=256, freeze_cnn=True, freeze_text_epochs=2):\n        super().__init__()\n\n        self.freeze_text_epochs = freeze_text_epochs\n\n        # ---------------------------------------------------------\n        # IMAGE ENCODER (RESNET50)\n        # ---------------------------------------------------------\n        self.cnn = models.resnet50(weights=\"IMAGENET1K_V2\")\n        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, embed_dim)\n\n        # Freeze CNN initially\n        if freeze_cnn:\n            for p in self.cnn.parameters():\n                p.requires_grad = False\n            for p in self.cnn.fc.parameters():\n                p.requires_grad = True\n\n        # ---------------------------------------------------------\n        # TEXT ENCODER (DISTILBERT)\n        # ---------------------------------------------------------\n        self.text_encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n        self.text_proj = nn.Linear(768, embed_dim)\n\n        # Freeze text encoder initially\n        for p in self.text_encoder.parameters():\n            p.requires_grad = False\n\n    def unfreeze_text_encoder(self):\n        print(\"Unfreezing Text Encoder…\")\n        for p in self.text_encoder.parameters():\n            p.requires_grad = True\n\n    def forward(self, images=None, input_ids=None, attention_mask=None):\n        img_features = None\n        txt_features = None\n\n        if images is not None:\n            img_features = self.cnn(images)\n            img_features = F.normalize(img_features, p=2, dim=1)\n\n        if input_ids is not None:\n            txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n            txt_features = txt_out.last_hidden_state[:, 0, :]\n            txt_features = self.text_proj(txt_features)\n            txt_features = F.normalize(txt_features, p=2, dim=1)\n\n        return img_features, txt_features\n","metadata":{"id":"HWUzT3vRGT6H","outputId":"2fcbcffe-d1f5-42af-834a-551ebc156ec8","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:38:46.643055Z","iopub.execute_input":"2025-12-01T00:38:46.643394Z","iopub.status.idle":"2025-12-01T00:38:46.651427Z","shell.execute_reply.started":"2025-12-01T00:38:46.643370Z","shell.execute_reply":"2025-12-01T00:38:46.650529Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ---------------------------------------------------------\n# TRAINING FUNCTION\n# ---------------------------------------------------------\ndef train_model(\n    model,\n    train_loader,\n    val_loader,\n    epochs=20,\n    lr=1e-4,\n    patience=4,\n    unfreeze_cnn_epoch=3,\n    temperature=0.07,\n    save_path=f\"{PROJECT_ROOT}/best_model.pt\"\n):\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=epochs,\n        eta_min=lr * 0.1\n    )\n\n    best_val_loss = float(\"inf\")\n    patience_counter = 0\n\n    def contrastive_loss(img_emb, txt_emb, temperature=temperature):\n        logits = (img_emb @ txt_emb.T) / temperature\n        logits = torch.clamp(logits, -50, 50)\n\n        labels = torch.arange(len(logits)).to(device)\n        loss_i = nn.CrossEntropyLoss()(logits, labels)\n        loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n\n        return (loss_i + loss_t) / 2\n\n    print(\"Starting Training...\")\n\n    for epoch in range(epochs):\n\n        # Unfreeze text encoder AFTER warmup\n        if epoch == model.freeze_text_epochs:\n            model.unfreeze_text_encoder()\n\n        # Unfreeze CNN later\n        if epoch == unfreeze_cnn_epoch:\n            print(f\"Unfreezing CNN at epoch {epoch}\")\n            for p in model.cnn.parameters():\n                p.requires_grad = True\n\n        model.train()\n        total_train_loss = 0\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n        for batch in loop:\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            mask = batch['attention_mask'].to(device)\n\n            img_emb, txt_emb = model(images, input_ids, mask)\n            loss = contrastive_loss(img_emb, txt_emb)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            loop.set_postfix(loss=loss.item())\n\n        avg_train_loss = total_train_loss / len(train_loader)\n\n        # ------------------------ VALIDATION ------------------------\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                images = batch['image'].to(device)\n                input_ids = batch['input_ids'].to(device)\n                mask = batch['attention_mask'].to(device)\n\n                img_emb, txt_emb = model(images, input_ids, mask)\n                total_val_loss += contrastive_loss(img_emb, txt_emb).item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n        # ---------------------- EARLY STOPPING ----------------------\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), save_path)\n            print(\"Saved NEW BEST MODEL!\")\n        else:\n            patience_counter += 1\n            print(f\"No improvement ({patience_counter}/{patience})\")\n\n            if patience_counter >= patience:\n                print(\"Early Stopping Triggered.\")\n                break\n\n        scheduler.step()\n\n    print(\"Training Finished.\")\n    print(f\"Best model saved at: {save_path}\")\n","metadata":{"id":"is70hEfXHWuS","outputId":"c99c6009-0762-4366-edb6-e4b638b99c14","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:38:34.690138Z","iopub.execute_input":"2025-12-01T00:38:34.691115Z","iopub.status.idle":"2025-12-01T00:38:34.702166Z","shell.execute_reply.started":"2025-12-01T00:38:34.691085Z","shell.execute_reply":"2025-12-01T00:38:34.701469Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# TRAINING CALL\nmodel = DualEncoder(embed_dim=256, freeze_cnn=True).to(device)\n\ntrain_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    epochs=20,\n    lr=1e-4,\n    patience=4,\n    unfreeze_cnn_epoch=3,\n    save_path=\"/kaggle/working/best_model.pt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:49:19.975902Z","iopub.execute_input":"2025-11-29T17:49:19.976686Z","iopub.status.idle":"2025-11-29T21:34:06.875145Z","shell.execute_reply.started":"2025-11-29T17:49:19.976659Z","shell.execute_reply":"2025-11-29T21:34:06.874202Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 205MB/s]\n2025-11-29 17:49:28.049562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764438568.236157      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764438568.291638      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd03a18c630a4c1fb4ba583787cd9807"}},"metadata":{}},{"name":"stdout","text":"Starting Training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 613/613 [09:43<00:00,  1.05it/s, loss=2.12]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/20\nTrain Loss: 3.5674 | Val Loss: 3.1563\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 613/613 [07:30<00:00,  1.36it/s, loss=2.04]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/20\nTrain Loss: 3.1340 | Val Loss: 2.9619\nSaved NEW BEST MODEL!\nUnfreezing Text Encoder…\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 613/613 [08:49<00:00,  1.16it/s, loss=0.798]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/20\nTrain Loss: 2.3062 | Val Loss: 2.0317\nSaved NEW BEST MODEL!\nUnfreezing CNN at epoch 3\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 613/613 [13:07<00:00,  1.28s/it, loss=0.348]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/20\nTrain Loss: 1.3932 | Val Loss: 1.3565\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 613/613 [13:06<00:00,  1.28s/it, loss=0.282]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/20\nTrain Loss: 0.8391 | Val Loss: 1.1703\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 613/613 [13:07<00:00,  1.29s/it, loss=0.227]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/20\nTrain Loss: 0.5529 | Val Loss: 1.0761\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 613/613 [13:06<00:00,  1.28s/it, loss=0.118]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/20\nTrain Loss: 0.3817 | Val Loss: 1.0676\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 613/613 [13:07<00:00,  1.29s/it, loss=0.0576]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/20\nTrain Loss: 0.2769 | Val Loss: 1.0204\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 613/613 [13:06<00:00,  1.28s/it, loss=0.0237]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/20\nTrain Loss: 0.2111 | Val Loss: 1.0447\nNo improvement (1/4)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 613/613 [13:06<00:00,  1.28s/it, loss=0.0177]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/20\nTrain Loss: 0.1652 | Val Loss: 1.0024\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 613/613 [13:07<00:00,  1.28s/it, loss=0.0639]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/20\nTrain Loss: 0.1364 | Val Loss: 1.0014\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 613/613 [13:07<00:00,  1.28s/it, loss=0.0676]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/20\nTrain Loss: 0.1137 | Val Loss: 1.0036\nNo improvement (1/4)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 613/613 [13:07<00:00,  1.28s/it, loss=0.0296]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/20\nTrain Loss: 0.0950 | Val Loss: 0.9867\nSaved NEW BEST MODEL!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 613/613 [13:06<00:00,  1.28s/it, loss=0.0179]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/20\nTrain Loss: 0.0814 | Val Loss: 1.0145\nNo improvement (1/4)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 613/613 [13:07<00:00,  1.28s/it, loss=0.00539]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/20\nTrain Loss: 0.0721 | Val Loss: 1.0078\nNo improvement (2/4)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 613/613 [13:07<00:00,  1.28s/it, loss=0.0116]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/20\nTrain Loss: 0.0655 | Val Loss: 1.0153\nNo improvement (3/4)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 613/613 [13:06<00:00,  1.28s/it, loss=0.00978]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/20\nTrain Loss: 0.0578 | Val Loss: 1.0206\nNo improvement (4/4)\nEarly Stopping Triggered.\nTraining Finished.\nBest model saved at: /kaggle/working/best_model.pt\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# **Evaluation**","metadata":{"id":"FPap5C5mOm3I"}},{"cell_type":"code","source":"# Load the Best Saved Model\nmodel = DualEncoder(embed_dim=256, freeze_cnn=False).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/best-model-100k/best_model (1).pt\", map_location=device))\nmodel.eval()\nprint(\"Best model loaded for testing.\")\n\n\n# 1. COMPUTE EMBEDDINGS FOR TEST SET\nimage_embeddings = []\ntext_embeddings = []\ntest_recipe_ids = []\n\nprint(\"Computing embeddings for Test Set\")\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n\n        # Get embeddings\n        img_emb, txt_emb = model(images, input_ids, mask)\n\n        # Store them on CPU to save GPU memory\n        image_embeddings.append(img_emb.cpu())\n        text_embeddings.append(txt_emb.cpu())\n        test_recipe_ids.extend(batch['recipe_id'])\n\n# Concatenate all batches\nimage_embeddings = torch.cat(image_embeddings)\ntext_embeddings = torch.cat(text_embeddings)\n\nprint(f\"Test Embeddings Shape: {image_embeddings.shape}\")\n\n\n# 2. CALCULATE COSINE SIMILARITY MATRIX\n\n# Normalize\nimage_embeddings = F.normalize(image_embeddings, p=2, dim=1)\ntext_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n\n# Similarity Matrix: [N_test x N_test]\n# row i, col j = similarity between Image_i and Text_j\nsimilarity_matrix = image_embeddings @ text_embeddings.T\n\n# similarity_matrix shape: [N_images, N_texts]\n\n\n# IMAGE → TEXT RETRIEVAL\n\n\ndef recall_at_k_i2t(similarity_matrix, k=1):\n    N = similarity_matrix.shape[0]\n    correct = 0\n\n    for i in range(N):\n        sims = similarity_matrix[i]\n        topk_idx = torch.topk(sims, k).indices.tolist()\n        if i in topk_idx:\n            correct += 1\n\n    return correct / N\n\n\ndef median_rank_i2t(similarity_matrix):\n    N = similarity_matrix.shape[0]\n    ranks = []\n\n    for i in range(N):\n        sims = similarity_matrix[i]\n        sorted_idx = torch.argsort(sims, descending=True)\n        rank = (sorted_idx == i).nonzero(as_tuple=False).item() + 1\n        ranks.append(rank)\n\n    return float(np.median(ranks))\n\n\n\n# TEXT → IMAGE RETRIEVAL\n\n\ndef recall_at_k_t2i(similarity_matrix, k=1):\n    sim_t2i = similarity_matrix.T\n    N = sim_t2i.shape[0]\n    correct = 0\n\n    for i in range(N):\n        sims = sim_t2i[i]\n        topk_idx = torch.topk(sims, k).indices.tolist()\n        if i in topk_idx:\n            correct += 1\n\n    return correct / N\n\n\ndef median_rank_t2i(similarity_matrix):\n    sim_t2i = similarity_matrix.T\n    N = sim_t2i.shape[0]\n    ranks = []\n\n    for i in range(N):\n        sims = sim_t2i[i]\n        sorted_idx = torch.argsort(sims, descending=True)\n        rank = (sorted_idx == i).nonzero(as_tuple=False).item() + 1\n        ranks.append(rank)\n\n    return float(np.median(ranks))\n\n\n\n# PRINT RESULTS\n\nprint(\"\\n\")\nprint(\"IMAGE → TEXT RETRIEVAL RESULTS\")\n\nfor k in [1, 5, 10]:\n    print(f\"Recall@{k}: {recall_at_k_i2t(similarity_matrix, k):.4f}\")\n\nprint(f\"Median Rank (MedR): {median_rank_i2t(similarity_matrix):.2f}\")\n\n\nprint(\"\\n\")\nprint(\"TEXT → IMAGE RETRIEVAL RESULTS\")\n\nfor k in [1, 5, 10]:\n    print(f\"Recall@{k}: {recall_at_k_t2i(similarity_matrix, k):.4f}\")\n\nprint(f\"Median Rank (MedR): {median_rank_t2i(similarity_matrix):.2f}\")\n","metadata":{"id":"TgOPSC-FIykf","outputId":"a2099ee8-89d0-4a82-a6d1-137ee186fe99","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:39:58.931505Z","iopub.execute_input":"2025-12-01T00:39:58.931814Z","iopub.status.idle":"2025-12-01T00:41:14.697836Z","shell.execute_reply.started":"2025-12-01T00:39:58.931793Z","shell.execute_reply":"2025-12-01T00:41:14.696842Z"}},"outputs":[{"name":"stdout","text":"Best model loaded for testing.\nComputing embeddings for Test Set\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 77/77 [01:08<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Embeddings Shape: torch.Size([4899, 256])\n\n\nIMAGE → TEXT RETRIEVAL RESULTS\nRecall@1: 0.1664\nRecall@5: 0.3882\nRecall@10: 0.4997\nMedian Rank (MedR): 11.00\n\n\nTEXT → IMAGE RETRIEVAL RESULTS\nRecall@1: 0.1678\nRecall@5: 0.3911\nRecall@10: 0.4979\nMedian Rank (MedR): 11.00\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# **Nutrition Optimization (Dietary constraint + Goals)**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport zipfile\nimport os\nimport re\n\nNUTRITION_EXTRACT_DIR = \"/kaggle/input/nutrition-data\"\n\nif not os.path.exists(NUTRITION_EXTRACT_DIR):\n    print(f\"Unzipping {NUTRITION_ZIP_PATH}\")\n    try:\n        with zipfile.ZipFile(NUTRITION_ZIP_PATH, 'r') as z:\n            z.extractall(NUTRITION_EXTRACT_DIR)\n        print(\"Extraction complete.\")\n    except Exception as e:\n        print(f\"Error unzipping: {e}\")\nelse:\n    print(\"Folder already exists, skipping unzip.\")\n\n\n# 2. FIND ALL DATA CSVs\ndata_csv_files = []\nfor root, dirs, files in os.walk(NUTRITION_EXTRACT_DIR):\n    for file in files:\n        if file.endswith(\".csv\") and \"FOOD-DATA\" in file and \"METADATA\" not in file:\n            data_csv_files.append(os.path.join(root, file))\n\nif not data_csv_files:\n    raise FileNotFoundError(\"No suitable CSV files found!\")\n\nprint(f\"Found {len(data_csv_files)} data files. Merging...\")\n\n# 3. LOAD & MERGE KAGGLE NUTRITION DATA\n\ndef get_val(row, possible_cols):\n    \"\"\"Helper to find the first valid column that exists\"\"\"\n    for col in possible_cols:\n        if col in row:\n            return float(row[col])\n    return 0.0\n\ndef load_all_nutrition_files(file_list):\n    lookup = {}\n\n    for csv_path in file_list:\n        try:\n            df = pd.read_csv(csv_path)\n            # Normalize columns\n            df.columns = [c.strip().lower().replace(\" \", \"_\").replace(\".\", \"\") for c in df.columns]\n\n            #Iterate rows\n            for _, row in df.iterrows():\n                # Get raw name\n                raw_name = str(row.get('food', '')).lower()\n                # Clean name: \"Chicken, raw\" -> \"chicken raw\"\n                clean_name = ''.join(c for c in raw_name if c.isalnum() or c.isspace()).strip()\n\n                if not clean_name: continue\n\n                # Extract Macros using flexible column names\n                macros = {\n                    'calories': get_val(row, ['caloric_value', 'calories', 'energy_kcal']),\n                    'protein':  get_val(row, ['protein', 'protein_g']),\n                    'fat':      get_val(row, ['fat', 'total_lipid_fat']),\n                    'carbs':    get_val(row, ['carbohydrates', 'carbohydrate_by_difference']),\n                    'sugar':    get_val(row, ['sugars', 'sugars_total'])\n                }\n\n                # STRATEGY: SAVE MULTIPLE KEYS\n                # 1. Save the full name: \"chicken raw\"\n                lookup[clean_name] = macros\n\n                # Save the FIRST word: \"chicken\"\n                first_word = clean_name.split()[0]\n                if len(first_word) > 2 and first_word not in lookup:\n                     lookup[first_word] = macros\n\n        except Exception as e:\n            print(f\"Error reading {csv_path}: {e}\")\n\n    return lookup\n\nnutrition_lookup = load_all_nutrition_files(data_csv_files)\nprint(f\"Successfully loaded {len(nutrition_lookup)} lookup keys.\")\n\n# 4. HEURISTIC INGREDIENT PARSER\n#INGREDIENT PARSER\n\n\ndef calculate_approx_macros(ingredients_list):\n    \"\"\"\n    1. Multi-word matching (e.g., 'chicken breast', 'olive oil')\n    2. Whole-word matching using regex boundaries\n    3. Fallback to single-word matching\n    4. Avoids false positives (egg in eggplant)\n    \"\"\"\n\n    total = {'calories': 0.0, 'protein': 0.0, 'fat': 0.0, 'carbs': 0.0, 'sugar': 0.0}\n    matched_count = 0\n\n    # Pre-clean all nutrition keys for quick search\n    nutrition_keys = list(nutrition_lookup.keys())\n\n    for ingredient in ingredients_list:\n        ingredient_clean = ingredient.lower()\n\n        # 1. MULTI-WORD MATCHING\n        match_found = False\n\n        for food_key in nutrition_keys:\n            # multi-word exact phrase match\n            pattern = r\"\\b\" + re.escape(food_key) + r\"\\b\"\n            if re.search(pattern, ingredient_clean):\n                macros = nutrition_lookup[food_key]\n                for k in total:\n                    total[k] += macros[k]\n                matched_count += 1\n                match_found = True\n                break\n\n        if match_found:\n            continue  # skip token matching if phrase match worked\n\n        # 2. SINGLE-WORD TOKEN MATCHING (fallback)\n        \n        # extract only alphabetical tokens\n        tokens = re.findall(r\"[a-zA-Z]+\", ingredient_clean)\n\n        for token in tokens:\n            if len(token) <= 2: \n                continue  # skip useless small words\n\n            # whole-word match\n            if token in nutrition_lookup:\n                macros = nutrition_lookup[token]\n                for k in total:\n                    total[k] += macros[k]\n                matched_count += 1\n                match_found = True\n                break\n\n    # 3. AVERAGE OVER MATCHED INGREDIENTS\n    if matched_count > 0:\n        for k in total:\n            total[k] = round(total[k] / matched_count, 2)\n    else:\n        # if nothing matched, return zeros\n        total = {k: 0.0 for k in total}\n\n    return total, matched_count","metadata":{"id":"hYnxrqX9OwwQ","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:42:33.190455Z","iopub.execute_input":"2025-12-01T00:42:33.190821Z","iopub.status.idle":"2025-12-01T00:42:33.513656Z","shell.execute_reply.started":"2025-12-01T00:42:33.190792Z","shell.execute_reply":"2025-12-01T00:42:33.512753Z"}},"outputs":[{"name":"stdout","text":"Folder already exists, skipping unzip.\nFound 5 data files. Merging...\nSuccessfully loaded 3112 lookup keys.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Nutrition","metadata":{"id":"XsAMnjjPQCjS"}},{"cell_type":"code","source":"# DIETARY FILTER\nimport re\n\ndef check_dietary_compliance(ingredients_list, diet_filter, title=\"\"):\n    \"\"\"\n    Dietary filter:\n    - Case-insensitive\n    - Whole-word boundaries (\\bX\\b)\n    - Multi-word detection\n    - Checks BOTH ingredients + title\n    \"\"\"\n\n    diet = diet_filter.lower().strip()\n\n    # Combine ingredients and title into a unified text blob\n    text_blob = (title + \" \" + \" \".join(ingredients_list)).lower()\n\n    # FOOD CATEGORY LISTS\n    MEAT = [\n        \"chicken\", \"chicken breast\", \"ground chicken\",\n        \"beef\", \"ground beef\", \"steak\", \"veal\",\n        \"mutton\", \"lamb\", \"goat\", \"buff\"\n        \"pork\", \"bacon\", \"ham\", \"pepperoni\", \"salami\", \"Pork\"\n        \"sausage\", \"turkey\", \"ground chuck\", \"chuck\", \"hen\", \"tenderloin\", \"sirloin\"\n    ]\n\n    FISH = [\n        \"fish\", \"seafood\", \"mixed seafood\", \"frozen seafood\",\n        \"seafood mix\", \"seafood medley\",\n        \"salmon\", \"tuna\", \"shrimp\", \"prawn\", \"anchovy\", \"cod\",\n        \"crab\", \"lobster\", \"scallop\", \"mussel\", \"clam\", \"oyster\",\n        \"squid\", \"octopus\", \"mackerel\", \"trout\", \"snapper\", \"scallops\", \"clams\"\n    ]\n\n    DAIRY = [\n        \"milk\", \"cheese\", \"butter\", \"yogurt\",\n        \"cream\", \"parmesan\", \"paneer\", \"ghee\", \"whey\"\n    ]\n\n    EGGS = [\"egg\", \"egg yolk\", \"eggs\"]\n\n    HONEY = [\"honey\"]\n\n    PORK = [\"pork\", \"bacon\", \"ham\", \"salami\", \"pepperoni\", \"lard\", \"chorizo\", \"Pork\"]\n\n    ALCOHOL = [\"wine\", \"vodka\", \"rum\", \"beer\", \"whiskey\", \"bourbon\", \"liqueur\"]\n\n    GLUTEN = [\"wheat\", \"flour\", \"bread\", \"noodles\", \"pasta\", \"spaghetti\", \"barley\", \"rye\", \"soy sauce\"]\n\n    # Helpers for whole-word matching\n    def contains(words):\n        for w in words:\n            pattern = r\"\\b\" + re.escape(w) + r\"\\b\"\n            if re.search(pattern, text_blob):\n                return True\n        return False\n\n    # -----------------------------\n    # APPLY DIET RULES\n    # -----------------------------\n    \n    # Vegan: no meat, fish, dairy, eggs, honey\n    if diet == \"vegan\":\n        if contains(MEAT+FISH+DAIRY+EGGS+HONEY):\n            return False\n\n    # Vegetarian: no meat, no fish\n    if diet == \"vegetarian\":\n        if contains(MEAT+FISH+PORK+EGGS):\n            return False\n\n    # Pescatarian: no meat, fish allowed\n    if diet == \"pescatarian\":\n        if contains(MEAT):\n            return False\n\n    # Halal: no pork, no alcohol\n    if diet == \"halal\":\n        if contains(PORK+ALCOHOL):\n            return False\n\n    # Gluten free\n    if diet == \"gluten free\":\n        if contains(GLUTEN):\n            return False\n    return True\n","metadata":{"id":"TuXRZzEqP_Ar","outputId":"8146a5e1-2933-4ca8-f7ee-8dfd1295938d","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:42:46.998695Z","iopub.execute_input":"2025-12-01T00:42:46.998996Z","iopub.status.idle":"2025-12-01T00:42:47.008748Z","shell.execute_reply.started":"2025-12-01T00:42:46.998975Z","shell.execute_reply":"2025-12-01T00:42:47.007889Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"#BUILD FULL DATABASE INDEX (Train + Val + Test)\n\n# 1. Prepare Data\nall_recipe_ids = [item['recipe_id'] for item in full_data]\nfull_db_embeddings = []\n\n# 2. Create a DataLoader for the Full Dataset (Text Only)\n# We can reuse the RecipeDataset class but we only need text\nfull_dataset = RecipeDataset(full_data, FINAL_IMG_DIR, val_transform, tokenizer)\nfull_loader = DataLoader(full_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# 3. Compute Embeddings\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(full_loader, desc=\"Indexing Full DB\"):\n        input_ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n\n        # Get Text Embeddings from the Model\n        # We manually call the text branch since we don't have images here\n        txt_out = model.text_encoder(input_ids=input_ids, attention_mask=mask)\n        txt_feat = txt_out.last_hidden_state[:, 0, :]\n        txt_feat = model.text_proj(txt_feat)\n        txt_feat = F.normalize(txt_feat, p=2, dim=1)\n\n        full_db_embeddings.append(txt_feat.cpu())\n\n# 4. Concatenate and move to device once, making it a global variable\nglobal db_emb\ndb_emb = torch.cat(full_db_embeddings).to(device)\nprint(f\"Full Database Indexed. Shape: {db_emb.shape}\")","metadata":{"id":"Kp0z0UGVSenw","outputId":"c8d6c902-d21f-439e-fc87-0ec8d1bfbaaf","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:42:56.671359Z","iopub.execute_input":"2025-12-01T00:42:56.672041Z","iopub.status.idle":"2025-12-01T00:54:03.000790Z","shell.execute_reply.started":"2025-12-01T00:42:56.672018Z","shell.execute_reply":"2025-12-01T00:54:03.000115Z"}},"outputs":[{"name":"stderr","text":"Indexing Full DB: 100%|██████████| 766/766 [11:06<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Database Indexed. Shape: torch.Size([48982, 256])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom PIL import Image as PILImage\n\n#SETUP\nif 'id_to_entry' not in globals():\n    id_to_entry = {item['recipe_id']: item for item in full_data}\n\n# RETRIEVAL FUNCTION\ndef retrieve_recipe_from_image(image_path, topk=50):\n    if topk is None: topk = 50\n    try:\n        img = PILImage.open(image_path).convert(\"RGB\")\n        img_t = val_transform(img).unsqueeze(0).to(device)\n    except:\n        return []\n\n    with torch.no_grad():\n        img_emb, _ = model(images=img_t)\n        img_emb = F.normalize(img_emb, p=2, dim=1)\n\n    sims = (img_emb @ db_emb.T).squeeze(0)\n\n    max_k = min(int(topk), len(all_recipe_ids))\n    top_indices = torch.topk(sims, k=max_k).indices.tolist()\n\n    results = []\n    for idx in top_indices:\n        rid = all_recipe_ids[idx]\n        entry = id_to_entry.get(rid)\n        if entry:\n            results.append({\n                \"recipe_id\": rid,\n                \"similarity\": sims[idx].item(),\n                \"image_filename\": entry['image_filename'],\n                \"title\": entry['title']\n            })\n    return results","metadata":{"id":"JB-GlkqmyBbC","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T01:09:03.492721Z","iopub.execute_input":"2025-12-01T01:09:03.493025Z","iopub.status.idle":"2025-12-01T01:09:03.500821Z","shell.execute_reply.started":"2025-12-01T01:09:03.493001Z","shell.execute_reply":"2025-12-01T01:09:03.499993Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# NORMALIZED NUTRITION SCORING\n\n# Convert nutrition lookup (all individual foods) into a DataFrame\nall_macros = pd.DataFrame(nutrition_lookup.values())\n\n# Fix column names\nall_macros = all_macros.rename(columns={\n    \"sugar\": \"sugar\"  # ensure consistent naming\n})\n\nmax_cal = all_macros[\"calories\"].max()\nmax_pro = all_macros[\"protein\"].max()\nmax_fat = all_macros[\"fat\"].max()\nmax_carb = all_macros[\"carbs\"].max()\nmax_sug = all_macros[\"sugar\"].max()\n\nprint(\"Max macro values computed.\")\n\ndef nutrition_objective_score(macros, objective):\n    if macros is None:\n        return 0.0\n\n    cal  = macros.get(\"calories\", 0)\n    pro  = macros.get(\"protein\", 0)\n    fat  = macros.get(\"fat\", 0)\n    carb = macros.get(\"carbs\", 0)\n    sug  = macros.get(\"sugar\", 0)\n\n    # Normalization\n    cal_n  = cal  / (max_cal  + 1e-6)\n    pro_n  = pro  / (max_pro  + 1e-6)\n    fat_n  = fat  / (max_fat  + 1e-6)\n    carb_n = carb / (max_carb + 1e-6)\n    sug_n  = sug  / (max_sug  + 1e-6)\n\n    # Objective scoring\n    if objective == \"high_protein\":\n        return pro_n\n\n    if objective == \"low_calorie\":\n        return 1.0 - cal_n\n\n    if objective == \"low_fat\":\n        return 1.0 - fat_n\n\n    if objective == \"low_carb\":\n        return 1.0 - carb_n\n\n    if objective == \"sugar_free\":\n        return 1.0 - sug_n\n\n    if objective == \"keto\":\n        return 0.7*(1-carb_n) + 0.3*(fat_n)\n\n    return 0.0","metadata":{"id":"E2I3X8gayDJ5","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T01:09:08.456334Z","iopub.execute_input":"2025-12-01T01:09:08.456608Z","iopub.status.idle":"2025-12-01T01:09:08.469686Z","shell.execute_reply.started":"2025-12-01T01:09:08.456587Z","shell.execute_reply":"2025-12-01T01:09:08.468829Z"}},"outputs":[{"name":"stdout","text":"Max macro values computed.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# RE-RANKING FUNCTION\ndef rerank_with_constraints(results, diet_type=\"none\", objective=\"none\", alpha=0.5):\n    final_list = []\n    \n    # sanitize inputs\n    diet_type = diet_type.lower()\n    objective = objective.lower()\n\n    for item in results:\n        rid = item[\"recipe_id\"]\n        entry = id_to_entry.get(rid)\n        if entry is None:\n            continue\n\n        # Compute macros\n        macros, _ = calculate_approx_macros(entry['ingredients'])\n\n        # Apply diet filter\n        if diet_type != \"none\":\n            if not check_dietary_compliance(entry['ingredients'], diet_type, entry['title']):\n                continue\n\n        # Nutrition score\n        nut_score = nutrition_objective_score(macros, objective)\n\n        # Visual similarity\n        sim = float(item[\"similarity\"])\n\n        # Final score (balanced)\n        final_score = (alpha * nut_score) + ((1 - alpha) * sim)\n\n        enriched = {\n            \"title\": entry.get(\"title\", \"Unknown\"),\n            \"image_filename\": entry.get(\"image_filename\", \"\"),\n            \"ingredients\": entry.get(\"ingredients\", []),\n            \"instructions\": entry.get(\"instructions\", []),\n            \"final_score\": final_score,\n            \"macros\": macros,\n            \"similarity\": sim\n        }\n        final_list.append(enriched)\n\n    # Sort descending\n    final_list.sort(key=lambda x: x[\"final_score\"], reverse=True)\n    return final_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T01:09:12.265163Z","iopub.execute_input":"2025-12-01T01:09:12.265809Z","iopub.status.idle":"2025-12-01T01:09:12.272756Z","shell.execute_reply.started":"2025-12-01T01:09:12.265782Z","shell.execute_reply":"2025-12-01T01:09:12.271872Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# TESTING ON IMAGE\nimport os\n\ntest_image_path = os.path.join(PROJECT_ROOT, \"/kaggle/input/images-for-test/creamy-mushroom-pasta-recipe-8.jpg\")\nprint(f\"Query Image Path: {test_image_path}\")\n\nprint(\"Step 1: Retrieving Top 10 visual matches...\")\nraw_results = retrieve_recipe_from_image(test_image_path, topk=200)\n\ndiet = \"vegetarian\"\ngoal = \"low_calorie\"\n\nprint(f\"Step 2: Re-ranking with Diet='{diet}' and Goal='{goal}'...\")\n\nranked_results = rerank_with_constraints(\n    raw_results,\n    diet_type=diet,\n    objective=goal,\n    alpha=0.7\n)\n\nprint(\"\\n\" + \"=\"*90)\nprint(f\"TOP 10 RESULTS FOR '{goal.upper()}'\")\nprint(\"=\"*90)\n\nprint(f\"{'RANK':<5} | {'SCORE':<8} | {'CAL':<6} | {'PRO':<6} | {'FAT':<6} | {'CARB':<6} | {'SUG':<6} | TITLE\")\nprint(\"-\" * 90)\n\nfor i, r in enumerate(ranked_results[:10]):\n    cal = r[\"macros\"][\"calories\"]\n    pro = r[\"macros\"][\"protein\"]\n    fat = r[\"macros\"][\"fat\"]\n    carb = r[\"macros\"][\"carbs\"]\n    sug = r[\"macros\"][\"sugar\"]\n\n    print(\n        f\"#{i+1:<4} | {r['final_score']:.4f} | \"\n        f\"{cal:<6.1f} | {pro:<6.1f} | {fat:<6.1f} | {carb:<6.1f} | {sug:<6.1f} | \"\n        f\"{r['title'][:40]}\"\n    )\n\nprint(\"=\"*90)\nprint(\"ℹNOTE: Final Score = α * NutritionScore + (1-α) * VisualSimilarity\")\n","metadata":{"id":"hFxyj4c2xAhN","outputId":"89fe1871-83af-4587-9924-e3781c4bb617","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T01:09:17.392224Z","iopub.execute_input":"2025-12-01T01:09:17.392525Z","iopub.status.idle":"2025-12-01T01:10:10.758579Z","shell.execute_reply.started":"2025-12-01T01:09:17.392502Z","shell.execute_reply":"2025-12-01T01:10:10.757776Z"}},"outputs":[{"name":"stdout","text":"Query Image Path: /kaggle/input/images-for-test/creamy-mushroom-pasta-recipe-8.jpg\nStep 1: Retrieving Top 10 visual matches...\nStep 2: Re-ranking with Diet='vegetarian' and Goal='low_calorie'...\n\n==========================================================================================\nTOP 10 RESULTS FOR 'LOW_CALORIE'\n==========================================================================================\nRANK  | SCORE    | CAL    | PRO    | FAT    | CARB   | SUG    | TITLE\n------------------------------------------------------------------------------------------\n#1    | 0.9303 | 59.1   | 1.0    | 2.4    | 7.3    | 0.5    | Linguine con Vongole\n#2    | 0.9094 | 104.6  | 1.5    | 4.5    | 15.0   | 9.7    | Fast and Easy Fettuccine\n#3    | 0.9074 | 71.9   | 1.6    | 3.0    | 9.7    | 0.6    | Spicy Spaghetti with Many Herbs\n#4    | 0.9038 | 74.9   | 0.8    | 5.5    | 4.1    | 0.5    | That's Shallota Flavor Spaghetti\n#5    | 0.9036 | 60.1   | 1.3    | 4.6    | 3.7    | 0.5    | Mushroom and Basil Pasta\n#6    | 0.9006 | 86.8   | 2.2    | 2.3    | 14.6   | 3.3    | Sesame - Ginger Noodles\n#7    | 0.9002 | 17.5   | 0.4    | 1.6    | 0.7    | 0.2    | Fettuccine with Sunchokes and Herbs\n#8    | 0.9001 | 75.3   | 0.3    | 4.7    | 4.1    | 2.0    | Stir-Fried Shirataki Noodles with Soy Sa\n#9    | 0.8981 | 25.4   | 0.1    | 2.7    | 0.4    | 0.1    | Spaghettini With Garlic, Red Pepper & Ex\n#10   | 0.8975 | 58.6   | 1.3    | 5.0    | 2.6    | 0.7    | Takuan (Yellow Pickled Daikon), Cucumber\n==========================================================================================\nℹNOTE: Final Score = α * NutritionScore + (1-α) * VisualSimilarity\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Interactive UI-Based Testing\n\nimport gradio as gr\nimport torch.nn.functional as F\nfrom PIL import Image as PILImage\nimport numpy as np\nimport os\n\n# FORMATTER\ndef format_multi_result(recipes, topk=2):\n    output_text = \"\"\n    for i, r in enumerate(recipes[:topk]):\n        m = r['macros']\n\n        output_text += f\"## Result {i+1}: {r['title']}\\n\\n\"\n\n        output_text += f\"**Score:** {r['final_score']:.3f}\\n\\n\"\n        output_text += (\n            f\"**Estimated Nutrition:** {m['calories']} cal | \"\n            f\"**{m['protein']}g Protein** | {m['fat']}g Fat | {m['carbs']}g Carbs\\n\\n\"\n        )\n\n        output_text += \"### 🥣 Ingredients\\n\"\n        ing_formatted = \"\\n\".join([f\"- {ing}\" for ing in r['ingredients'][:8]])\n        output_text += ing_formatted + \"\\n\"\n        if len(r['ingredients']) > 8:\n            output_text += f\"- ... ({len(r['ingredients']) - 8} more)\\n\"\n\n        output_text += \"\\n### 📝 Instructions\\n\"\n        inst_formatted = \"\\n\".join([f\"{j+1}. {inst}\" \n                                     for j, inst in enumerate(r['instructions'][:5])])\n        output_text += inst_formatted + \"\\n\"\n        if len(r['instructions']) > 5:\n            output_text += f\"- ... ({len(r['instructions']) - 5} more steps)\\n\"\n\n        output_text += \"\\n---\\n\\n\"\n    return output_text\n\n\n# ---------------------------------------------------------\n# MAIN GRADIO FUNCTION\n# ---------------------------------------------------------\ndef gradio_retrieve(img, diet_type, objective, topk):\n    if img is None:\n        return \"Please upload an image.\", None\n\n    img_path = \"/content/query_image.jpg\"\n    img.save(img_path)\n\n    raw = retrieve_recipe_from_image(img_path, topk=topk)\n\n    final = rerank_with_constraints(\n        raw,\n        diet_type=diet_type.lower().strip(),   \n        objective=objective.lower().strip(), \n        alpha=0.3\n    )\n\n    if not final:\n        return \"No recipes found.\", None\n\n    text_block = format_multi_result(final, topk=2)\n\n    gallery_data = []\n    for r in final[:3]:\n        img_path = os.path.join(FINAL_IMG_DIR, r[\"image_filename\"])\n        if os.path.exists(img_path):\n            gallery_data.append((PILImage.open(img_path), r[\"title\"]))\n\n    return text_block, gallery_data\n\n\n\n# GRADIO UI\n\nif __name__ == '__main__':\n\n    diet_opts = ['none','vegan','vegetarian','pescatarian','halal','gluten free']\n\n    goal_opts = ['none','high_protein','low_calorie','low_fat','low_carb',\n                 'sugar_free','keto']\n\n    image_input = gr.Image(type=\"pil\", label=\"Upload an image\")\n\n    diet_dropdown = gr.Dropdown(choices=diet_opts, value='none',\n                                label=\"Dietary Preference\")\n\n    objective_dropdown = gr.Dropdown(choices=goal_opts, value='none',\n                                     label=\"Nutritional Goal\")\n\n    topk_slider = gr.Slider(minimum=5, maximum=200, value=50, step=5,\n                             label=\"Retrieve Top-K Candidates\")\n\n    text_output = gr.Markdown(label=\"Recipe Details\")\n    gallery_output = gr.Gallery(label=\"Recipe Images\")\n\n    demo = gr.Interface(\n        fn=gradio_retrieve,\n        inputs=[image_input, diet_dropdown, objective_dropdown, topk_slider],\n        outputs=[text_output, gallery_output],\n        title=\"Recipe Retrieval (Image → Recipe + Nutrition + Diet Filters)\",\n        description=\"Upload a dish image and get matching recipes filtered by diet and nutritional goals.\"\n    )\n\n    demo.launch(debug=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"9YMrI74QdDeK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"byENp2-y2HVS","trusted":true},"outputs":[],"execution_count":null}]}
